{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meet Quality Analysis\n",
    "\n",
    "This notebook calculates meet competitiveness metrics (competitor count, average strength) and identifies elite meets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from utils import categorize_ipf_weightclass, map_division_to_age_group, categorize_age_group, create_quality_filter, categorize_lifters, categorize_federation_testing_status\n",
    "\n",
    "# Load the processed dataset\n",
    "df = pd.read_parquet(\"../data/processed/full_dataset.parquet\")\n",
    "print(f\"Dataset loaded: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
    "\n",
    "# Add IPF Weight Class categorization (needed for quality filter)\n",
    "if 'BodyweightKg' in df.columns and 'Sex' in df.columns:\n",
    "    df['IPF_WeightClass'] = df.apply(\n",
    "        lambda row: categorize_ipf_weightclass(row['BodyweightKg'], row['Sex']), \n",
    "        axis=1\n",
    "    )\n",
    "# Apply quality filter (excludes outliers and invalid data)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"APPLYING QUALITY FILTER\")\n",
    "print(\"=\"*80)\n",
    "quality_mask = create_quality_filter(df)\n",
    "filtered_out = (~quality_mask).sum()\n",
    "print(f\"Quality filter will exclude {filtered_out:,} entries ({filtered_out/len(df)*100:.2f}%)\")\n",
    "print(f\"  Remaining entries for analysis: {quality_mask.sum():,} ({quality_mask.sum()/len(df)*100:.2f}%)\")\n",
    "# Create clean dataset (filtered)\n",
    "df = df[quality_mask].copy()\n",
    "# Add Age Group categorization from Division (100% coverage)\n",
    "if 'Division' in df.columns:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ADDING AGE GROUP CATEGORIZATION\")\n",
    "    print(\"=\"*80)\n",
    "    df['AgeGroup'] = df['Division'].apply(map_division_to_age_group)\n",
    "    print(\"Age Group categorization complete (using Division - 100% coverage)\")\n",
    "    if 'AgeGroup' in df.columns:\n",
    "        print(f\"Age Group distribution:\\n{df['AgeGroup'].value_counts()}\")\n",
    "else:\n",
    "    print(\"Warning: Cannot add Age Group - missing Division column\")\n",
    "    df['AgeGroup'] = None\n",
    "print(f\"\\n✓ Using filtered dataset with {len(df):,} entries\")\n",
    "# Categorize lifters (New/Intermediate/Advanced)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CATEGORIZING LIFTERS\")\n",
    "print(\"=\"*80)\n",
    "df = categorize_lifters(df)\n",
    "print(\"✓ Lifters categorized into New, Intermediate, and Advanced\")\n",
    "\n",
    "# Categorize federation testing status (Drug Tested vs Untested)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CATEGORIZING FEDERATION TESTING STATUS\")\n",
    "print(\"=\"*80)\n",
    "df = categorize_federation_testing_status(df)\n",
    "testing_status_counts = df['FederationTestingStatus'].value_counts()\n",
    "print(\"Federation Testing Status distribution:\")\n",
    "print(testing_status_counts)\n",
    "print(f\"✓ Federation testing status categorized\")\n",
    "# Create separate datasets for each category\n",
    "if 'LifterCategory' in df.columns:\n",
    "    new_lifters_df = df[df['LifterCategory'] == 'New'].copy()\n",
    "    intermediate_lifters_df = df[df['LifterCategory'] == 'Intermediate'].copy()\n",
    "    advanced_lifters_df = df[df['LifterCategory'] == 'Advanced'].copy()\n",
    "    \n",
    "    print(f\"\\nCategory breakdown:\")\n",
    "    print(f\"  New lifters: {len(new_lifters_df):,} entries\")\n",
    "    print(f\"  Intermediate lifters: {len(intermediate_lifters_df):,} entries\")\n",
    "    print(f\"  Advanced lifters: {len(advanced_lifters_df):,} entries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the processed dataset\n",
    "df = pd.read_parquet(\"../data/processed/full_dataset.parquet\")\n",
    "print(f\"Dataset loaded: {df.shape[0]:,} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Calculate Meet-Level Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Dots or Wilks as base score if available\n",
    "if 'Dots' in df.columns:\n",
    "    base_score_col = 'Dots'\n",
    "elif 'Wilks' in df.columns:\n",
    "    base_score_col = 'Wilks'\n",
    "else:\n",
    "    base_score_col = 'TotalKg'\n",
    "\n",
    "# Filter to valid performance data\n",
    "analysis_df = df[(df[base_score_col].notna()) & (df[base_score_col] > 0)].copy()\n",
    "\n",
    "print(f\"Using {base_score_col} as performance metric\")\n",
    "print(f\"Analysis dataset: {len(analysis_df):,} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-meet statistics\n",
    "if 'MeetPath' in analysis_df.columns:\n",
    "    meet_stats = analysis_df.groupby('MeetPath').agg({\n",
    "        base_score_col: ['count', 'mean', 'median', 'std', 'min', 'max'],\n",
    "        'Name': 'nunique',  # Number of unique competitors\n",
    "        'Federation': 'first',\n",
    "        'MeetDate': 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten column names\n",
    "    meet_stats.columns = ['MeetPath', 'CompetitorCount', 'AvgScore', 'MedianScore', \n",
    "                         'StdScore', 'MinScore', 'MaxScore', 'UniqueLifters', \n",
    "                         'Federation', 'MeetDate']\n",
    "    \n",
    "    # Calculate top competitor strength (top 10% average)\n",
    "    top_10_pct_scores = []\n",
    "    for meet_path in meet_stats['MeetPath']:\n",
    "        meet_data = analysis_df[analysis_df['MeetPath'] == meet_path][base_score_col]\n",
    "        if len(meet_data) > 0:\n",
    "            top_10 = meet_data.nlargest(max(1, int(len(meet_data) * 0.1)))\n",
    "            top_10_pct_scores.append(top_10.mean())\n",
    "        else:\n",
    "            top_10_pct_scores.append(np.nan)\n",
    "    \n",
    "    meet_stats['Top10PctAvg'] = top_10_pct_scores\n",
    "    \n",
    "    print(f\"Calculated statistics for {len(meet_stats):,} meets\")\n",
    "    print(f\"\\nMeet statistics summary:\")\n",
    "    print(meet_stats[['CompetitorCount', 'AvgScore', 'MedianScore', 'Top10PctAvg']].describe())\n",
    "else:\n",
    "    print(\"MeetPath column not found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Identify Elite Meets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify elite meets (top 10% by average competitor strength)\n",
    "elite_threshold = meet_stats['AvgScore'].quantile(0.90)\n",
    "elite_meets = meet_stats[meet_stats['AvgScore'] >= elite_threshold].copy()\n",
    "\n",
    "print(f\"=== Elite Meets Analysis ===\")\n",
    "print(f\"Elite threshold (90th percentile): {elite_threshold:.2f}\")\n",
    "print(f\"Number of elite meets: {len(elite_meets):,} ({len(elite_meets)/len(meet_stats)*100:.1f}%)\")\n",
    "print(f\"\\nElite meets statistics:\")\n",
    "print(elite_meets[['CompetitorCount', 'AvgScore', 'MedianScore', 'Top10PctAvg']].describe())\n",
    "\n",
    "# Top 20 elite meets\n",
    "print(f\"\\n=== Top 20 Elite Meets ===\")\n",
    "top_elite = elite_meets.nlargest(20, 'AvgScore')[['MeetPath', 'Federation', 'MeetDate', \n",
    "                                                   'CompetitorCount', 'AvgScore', 'Top10PctAvg']]\n",
    "print(top_elite.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize meet quality distribution (split by testing status)\n",
    "fig, axes = plt.subplots(4, 2, figsize=(16, 20))\n",
    "\n",
    "# Split visualizations by testing status\n",
    "for row_idx, testing_status in enumerate(['Drug Tested', 'Untested']):\n",
    "    status_meet_stats = meet_stats[meet_stats['FederationTestingStatus'] == testing_status]\n",
    "    \n",
    "    if len(status_meet_stats) > 0:\n",
    "        elite_threshold_status = status_meet_stats['AvgScore'].quantile(0.90)\n",
    "        elite_meets_status = status_meet_stats[status_meet_stats['AvgScore'] >= elite_threshold_status]\n",
    "        \n",
    "        # Distribution of average scores\n",
    "        ax1 = axes[row_idx*2, 0]\n",
    "        status_meet_stats['AvgScore'].hist(bins=50, edgecolor='black', alpha=0.7, ax=ax1)\n",
    "        ax1.axvline(elite_threshold_status, color='r', linestyle='--', label=f'Elite threshold: {elite_threshold_status:.1f}')\n",
    "        ax1.set_xlabel(f'Average {base_score_col}')\n",
    "        ax1.set_ylabel('Number of Meets')\n",
    "        ax1.set_title(f'Distribution of Average Competitor Strength - {testing_status}')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Competitor count vs average score\n",
    "        ax2 = axes[row_idx*2, 1]\n",
    "        sample_meets = status_meet_stats.sample(min(5000, len(status_meet_stats)))\n",
    "        ax2.scatter(sample_meets['CompetitorCount'], sample_meets['AvgScore'], alpha=0.3, s=10)\n",
    "        ax2.set_xlabel('Number of Competitors')\n",
    "        ax2.set_ylabel(f'Average {base_score_col}')\n",
    "        ax2.set_title(f'Meet Size vs Average Strength - {testing_status}')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Top 10% average vs overall average\n",
    "        ax3 = axes[row_idx*2+1, 0]\n",
    "        ax3.scatter(status_meet_stats['AvgScore'], status_meet_stats['Top10PctAvg'], alpha=0.3, s=10)\n",
    "        ax3.plot([status_meet_stats['AvgScore'].min(), status_meet_stats['AvgScore'].max()], \n",
    "                [status_meet_stats['AvgScore'].min(), status_meet_stats['AvgScore'].max()], \n",
    "                'r--', label='y=x')\n",
    "        ax3.set_xlabel(f'Average {base_score_col}')\n",
    "        ax3.set_ylabel(f'Top 10% Average {base_score_col}')\n",
    "        ax3.set_title(f'Top Competitors vs Overall Average - {testing_status}')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Elite meets over time\n",
    "        ax4 = axes[row_idx*2+1, 1]\n",
    "        if 'MeetDate' in status_meet_stats.columns:\n",
    "            status_meet_stats['Year'] = pd.to_datetime(status_meet_stats['MeetDate']).dt.year\n",
    "            elite_meets_status['Year'] = pd.to_datetime(elite_meets_status['MeetDate']).dt.year\n",
    "            yearly_elite = elite_meets_status.groupby('Year').size()\n",
    "            yearly_total = status_meet_stats.groupby('Year').size()\n",
    "            ax4.plot(yearly_elite.index, yearly_elite.values, label='Elite meets', linewidth=2)\n",
    "            ax4.plot(yearly_total.index, yearly_total.values, label='Total meets', linewidth=2, alpha=0.5)\n",
    "            ax4.set_xlabel('Year')\n",
    "            ax4.set_ylabel('Number of Meets')\n",
    "            ax4.set_title(f'Elite Meets Over Time - {testing_status}')\n",
    "            ax4.legend()\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/meet_quality_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Federation Prestige Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate federation-level statistics (split by testing status)\n",
    "if 'Federation' in meet_stats.columns:\n",
    "    for testing_status in ['Drug Tested', 'Untested']:\n",
    "        status_meet_stats = meet_stats[meet_stats['FederationTestingStatus'] == testing_status]\n",
    "        \n",
    "        if len(status_meet_stats) > 0:\n",
    "            fed_stats = status_meet_stats.groupby('Federation').agg({\n",
    "                'AvgScore': ['mean', 'median', 'count'],\n",
    "                'CompetitorCount': 'mean',\n",
    "                'Top10PctAvg': 'mean'\n",
    "            }).reset_index()\n",
    "            \n",
    "            fed_stats.columns = ['Federation', 'MeanAvgScore', 'MedianAvgScore', 'MeetCount', \n",
    "                                'AvgCompetitorCount', 'AvgTop10Pct']\n",
    "            \n",
    "            # Filter to federations with at least 10 meets\n",
    "            fed_stats = fed_stats[fed_stats['MeetCount'] >= 10].sort_values('MeanAvgScore', ascending=False)\n",
    "            \n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"FEDERATION PRESTIGE RANKINGS (TOP 20) - {testing_status.upper()}\")\n",
    "            print(f\"{'='*80}\")\n",
    "            print(fed_stats.head(20).to_string(index=False))\n",
    "            \n",
    "            # Visualize top federations\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            top_feds = fed_stats.head(20)\n",
    "            plt.barh(range(len(top_feds)), top_feds['MeanAvgScore'])\n",
    "            plt.yticks(range(len(top_feds)), top_feds['Federation'])\n",
    "            plt.xlabel(f'Average {base_score_col}')\n",
    "            plt.title(f'Top 20 Federations by Average Competitor Strength - {testing_status}')\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'../data/processed/federation_prestige_{testing_status.replace(\" \", \"_\").lower()}.png', dpi=150, bbox_inches='tight')\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Key Findings Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Meet Quality Analysis Summary ===\")\n",
    "print(f\"\\n1. Total meets analyzed: {len(meet_stats):,}\")\n",
    "\n",
    "# Summary split by testing status\n",
    "for testing_status in ['Drug Tested', 'Untested']:\n",
    "    status_meet_stats = meet_stats[meet_stats['FederationTestingStatus'] == testing_status]\n",
    "    if len(status_meet_stats) > 0:\n",
    "        elite_threshold_status = status_meet_stats['AvgScore'].quantile(0.90)\n",
    "        elite_meets_status = status_meet_stats[status_meet_stats['AvgScore'] >= elite_threshold_status]\n",
    "        \n",
    "        print(f\"\\n{testing_status.upper()}:\")\n",
    "        print(f\"  2. Average competitors per meet: {status_meet_stats['CompetitorCount'].mean():.1f}\")\n",
    "        print(f\"  3. Average competitor strength: {status_meet_stats['AvgScore'].mean():.2f}\")\n",
    "        print(f\"  4. Elite meets threshold: {elite_threshold_status:.2f}\")\n",
    "        print(f\"  5. Number of elite meets: {len(elite_meets_status):,}\")\n",
    "\n",
    "# Save meet quality metrics\n",
    "meet_stats.to_parquet('../data/processed/meet_quality_metrics.parquet', index=False)\n",
    "print(\"\\nMeet quality metrics saved to ../data/processed/meet_quality_metrics.parquet\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
