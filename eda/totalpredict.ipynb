{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86a98d13",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7f4c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from multiprocessing import cpu_count\n",
    "import time\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    HAS_TQDM = True\n",
    "except ImportError:\n",
    "    HAS_TQDM = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe205a3",
   "metadata": {},
   "source": [
    "## Data Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80012dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading IPF: 100%|██████████| 532/532 [00:00<00:00, 825.09it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPF: Loaded 265 entries files, 265 meets files in 0.71 seconds\n",
      "IPF: Loaded 265 entries files, 265 meets files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading USAPL: 100%|██████████| 8420/8420 [00:10<00:00, 791.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USAPL: Loaded 4206 entries files, 4206 meets files in 10.71 seconds\n",
      "USAPL: Loaded 4206 entries files, 4206 meets files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading PA: 100%|██████████| 1904/1904 [00:02<00:00, 890.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PA: Loaded 951 entries files, 951 meets files in 2.17 seconds\n",
      "PA: Loaded 951 entries files, 951 meets files\n",
      "\n",
      "============================================================\n",
      "COMBINED RESULTS:\n",
      "============================================================\n",
      "Total entries: 362415\n",
      "Total meets: 5422\n",
      "\n",
      "Entries breakdown:\n",
      "  - IPF: 52706 entries\n",
      "  - USAPL: 284587 entries\n",
      "  - PA: 25122 entries\n",
      "\n",
      "Meets breakdown:\n",
      "  - IPF: 265 meets\n",
      "  - USAPL: 4206 meets\n",
      "  - PA: 951 meets\n",
      "\n",
      "Entries dataframe shape: (362415, 52)\n",
      "Meets dataframe shape: (5422, 7)\n",
      "Age still missing: 24583\n",
      "\n",
      "Age statistics:\n",
      "count    381957.000000\n",
      "mean         29.745970\n",
      "std          13.333483\n",
      "min           7.991786\n",
      "25%          20.000000\n",
      "50%          25.000000\n",
      "75%          36.000000\n",
      "max          94.000000\n",
      "Name: Age, dtype: float64\n",
      "\n",
      "============================================================\n",
      "AFTER MERGE:\n",
      "============================================================\n",
      "Entries dataframe shape: (406540, 58)\n",
      "Columns: ['Place', 'WeightClassKg', 'Sex', 'Division', 'Name', 'BirthYear', 'BodyweightKg', 'Country', 'Best3SquatKg', 'Best3BenchKg', 'Best3DeadliftKg', 'TotalKg', 'Bench4Kg', 'Event', 'Equipment', 'MeetID', 'Bench1Kg', 'Bench2Kg', 'Bench3Kg', 'BirthDate', 'Deadlift4Kg', 'Team', 'Squat4Kg', 'Squat1Kg', 'Squat2Kg', 'Squat3Kg', 'Deadlift1Kg', 'Deadlift2Kg', 'Deadlift3Kg', 'Age', 'State', 'Tested', 'AgeRange', 'DeadliftEquipment', 'CyrillicName', 'ChineseName', 'KoreanName', 'BodyweightLbs', 'WeightClassLbs', 'Squat1Lbs', 'Squat2Lbs', 'Squat3Lbs', 'Best3SquatLbs', 'Bench1Lbs', 'Bench2Lbs', 'Bench3Lbs', 'Best3BenchLbs', 'Deadlift1Lbs', 'Deadlift2Lbs', 'Deadlift3Lbs', 'Best3DeadliftLbs', 'TotalLbs', 'Federation', 'Date', 'MeetCountry', 'MeetState', 'MeetTown', 'MeetName']\n",
      "\n",
      "Date column info:\n",
      "  - Non-null dates: 406540\n",
      "  - Date range: 1973-11-09 00:00:00 to 2025-12-24 00:00:00\n",
      "\n",
      "First few rows with merged data:\n",
      "                Name MeetID       Date Division Sex WeightClassKg  \\\n",
      "0      Wei-Ling Chen   0105 2001-09-11  Juniors   F            44   \n",
      "1      Wei-Ling Chen   0105 2001-06-17  Juniors   F            44   \n",
      "2  Natalia Krikunova   0105 2001-09-11  Juniors   F            44   \n",
      "3  Natalia Krikunova   0105 2001-06-17  Juniors   F            44   \n",
      "4       Oxana Sirant   0105 2001-09-11  Juniors   F            44   \n",
      "\n",
      "   Best3SquatKg  Best3BenchKg  Best3DeadliftKg  \n",
      "0         150.0          65.0            160.0  \n",
      "1         150.0          65.0            160.0  \n",
      "2         122.5          70.0            132.5  \n",
      "3         122.5          70.0            132.5  \n",
      "4         120.0          52.5            145.0  \n"
     ]
    }
   ],
   "source": [
    "def load_single_file(args):\n",
    "    \"\"\"\n",
    "    Load a single CSV file. Designed for multiprocessing.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    args : tuple\n",
    "        (file_path, meet_id, file_type) where file_type is 'entries' or 'meet'\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame or None\n",
    "        Loaded dataframe with MeetID column, or None if file doesn't exist or fails\n",
    "    \"\"\"\n",
    "    file_path, meet_id, file_type = args\n",
    "    try:\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['MeetID'] = meet_id\n",
    "            return df\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_federation_data(federation_path, federation_name):\n",
    "    \"\"\"\n",
    "    Load entries.csv and meet.csv files from all subfolders in a federation directory.\n",
    "    Uses multiprocessing to load files in parallel.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    federation_path : str\n",
    "        Path to the federation folder (e.g., \"../opl-data/meet-data/ipf\")\n",
    "    federation_name : str\n",
    "        Name of the federation (for display purposes)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    entries_list : list\n",
    "        List of entries dataframes\n",
    "    meets_list : list\n",
    "        List of meets dataframes\n",
    "    \"\"\"\n",
    "    # Get all folders in federation directory\n",
    "    folders = [f for f in os.listdir(federation_path) \n",
    "               if os.path.isdir(os.path.join(federation_path, f)) \n",
    "               and not f.startswith('.')]\n",
    "    \n",
    "    # Prepare list of all files to load\n",
    "    file_tasks = []\n",
    "    for folder in folders:\n",
    "        entries_path = os.path.join(federation_path, folder, \"entries.csv\")\n",
    "        meet_path = os.path.join(federation_path, folder, \"meet.csv\")\n",
    "        file_tasks.append((entries_path, folder, 'entries'))\n",
    "        file_tasks.append((meet_path, folder, 'meet'))\n",
    "    \n",
    "    # Use threading to load files in parallel\n",
    "    # ThreadPoolExecutor is ideal for I/O-bound operations like reading CSV files\n",
    "    # Use 12 workers to match logical processor count (or cpu_count() for portability)\n",
    "    max_workers = min(12, cpu_count())\n",
    "    \n",
    "    start_time = time.time()\n",
    "    entries_list = []\n",
    "    meets_list = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all tasks\n",
    "        if HAS_TQDM:\n",
    "            futures = {executor.submit(load_single_file, task): task for task in file_tasks}\n",
    "            # Collect results with progress bar\n",
    "            for future in tqdm(as_completed(futures), total=len(file_tasks), desc=f\"Loading {federation_name}\"):\n",
    "                result = future.result()\n",
    "                task = futures[future]\n",
    "                if result is not None:\n",
    "                    if task[2] == 'entries':\n",
    "                        entries_list.append(result)\n",
    "                    else:\n",
    "                        meets_list.append(result)\n",
    "        else:\n",
    "            # Without tqdm, submit all tasks and collect results\n",
    "            futures = {executor.submit(load_single_file, task): task for task in file_tasks}\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                task = futures[future]\n",
    "                if result is not None:\n",
    "                    if task[2] == 'entries':\n",
    "                        entries_list.append(result)\n",
    "                    else:\n",
    "                        meets_list.append(result)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"{federation_name}: Loaded {len(entries_list)} entries files, {len(meets_list)} meets files in {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    return entries_list, meets_list\n",
    "\n",
    "# Load data from all three federations\n",
    "ipf_path = \"../opl-data/meet-data/ipf\"\n",
    "usapl_path = \"../opl-data/meet-data/usapl\"\n",
    "pa_path = \"../opl-data/meet-data/pa\"\n",
    "\n",
    "# Load IPF data\n",
    "ipf_entries, ipf_meets = load_federation_data(ipf_path, \"IPF\")\n",
    "print(f\"IPF: Loaded {len(ipf_entries)} entries files, {len(ipf_meets)} meets files\")\n",
    "\n",
    "# Load USAPL data\n",
    "usapl_entries, usapl_meets = load_federation_data(usapl_path, \"USAPL\")\n",
    "print(f\"USAPL: Loaded {len(usapl_entries)} entries files, {len(usapl_meets)} meets files\")\n",
    "\n",
    "# Load PA data\n",
    "pa_entries, pa_meets = load_federation_data(pa_path, \"PA\")\n",
    "print(f\"PA: Loaded {len(pa_entries)} entries files, {len(pa_meets)} meets files\")\n",
    "\n",
    "# Combine all entries from all federations\n",
    "all_entries_list = ipf_entries + usapl_entries + pa_entries\n",
    "all_meets_list = ipf_meets + usapl_meets + pa_meets\n",
    "\n",
    "# Combine all entries\n",
    "df_entries_all = pd.concat(all_entries_list, ignore_index=True)\n",
    "\n",
    "# Combine all meets\n",
    "df_meets_all = pd.concat(all_meets_list, ignore_index=True)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"COMBINED RESULTS:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total entries: {len(df_entries_all)}\")\n",
    "print(f\"Total meets: {len(df_meets_all)}\")\n",
    "print(f\"\\nEntries breakdown:\")\n",
    "print(f\"  - IPF: {len(pd.concat(ipf_entries, ignore_index=True)) if ipf_entries else 0} entries\")\n",
    "print(f\"  - USAPL: {len(pd.concat(usapl_entries, ignore_index=True)) if usapl_entries else 0} entries\")\n",
    "print(f\"  - PA: {len(pd.concat(pa_entries, ignore_index=True)) if pa_entries else 0} entries\")\n",
    "print(f\"\\nMeets breakdown:\")\n",
    "print(f\"  - IPF: {len(pd.concat(ipf_meets, ignore_index=True)) if ipf_meets else 0} meets\")\n",
    "print(f\"  - USAPL: {len(pd.concat(usapl_meets, ignore_index=True)) if usapl_meets else 0} meets\")\n",
    "print(f\"  - PA: {len(pd.concat(pa_meets, ignore_index=True)) if pa_meets else 0} meets\")\n",
    "print(f\"\\nEntries dataframe shape: {df_entries_all.shape}\")\n",
    "print(f\"Meets dataframe shape: {df_meets_all.shape}\")\n",
    "\n",
    "# Merge entries with meet data using MeetID\n",
    "# This adds Date and other meet information to each entry\n",
    "df_entries_all = df_entries_all.merge(\n",
    "    df_meets_all,\n",
    "    on='MeetID',\n",
    "    how='left',  # Keep all entries even if meet data is missing\n",
    "    suffixes=('', '_meet')  # In case of duplicate column names\n",
    ")\n",
    "\n",
    "# Convert Date to datetime \n",
    "df_entries_all['Date'] = pd.to_datetime(df_entries_all['Date'])\n",
    "\n",
    "# Convert BirthDate to datetime \n",
    "df_entries_all['BirthDate'] = pd.to_datetime(df_entries_all['BirthDate'], errors='coerce')\n",
    "\n",
    "# Calculate age in years: (Date - BirthDate) / 365.25\n",
    "# Using 365.25 to account for leap years\n",
    "df_entries_all['Age'] = (df_entries_all['Date'] - df_entries_all['BirthDate']).dt.days / 365.25\n",
    "\n",
    "# For entries where BirthDate is missing but BirthYear exists, estimate age\n",
    "# Use mid-year as approximation: Date.year - BirthYear\n",
    "missing_age_mask = df_entries_all['Age'].isna() & df_entries_all['BirthYear'].notna()\n",
    "df_entries_all.loc[missing_age_mask, 'Age'] = (\n",
    "    df_entries_all.loc[missing_age_mask, 'Date'].dt.year - \n",
    "    df_entries_all.loc[missing_age_mask, 'BirthYear']\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Age still missing: {df_entries_all['Age'].isna().sum()}\")\n",
    "print(f\"\\nAge statistics:\")\n",
    "print(df_entries_all['Age'].describe())\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"AFTER MERGE:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Entries dataframe shape: {df_entries_all.shape}\")\n",
    "print(f\"Columns: {list(df_entries_all.columns)}\")\n",
    "print(f\"\\nDate column info:\")\n",
    "print(f\"  - Non-null dates: {df_entries_all['Date'].notna().sum()}\")\n",
    "print(f\"  - Date range: {df_entries_all['Date'].min()} to {df_entries_all['Date'].max()}\")\n",
    "print(\"\\nFirst few rows with merged data:\")\n",
    "print(df_entries_all[['Name', 'MeetID', 'Date', 'Division','Sex','WeightClassKg','Best3SquatKg', 'Best3BenchKg', 'Best3DeadliftKg']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215ca1b5",
   "metadata": {},
   "source": [
    "## Convert Pounds to Kilograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adcd14fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled 251 missing values in Best3SquatKg using Best3SquatLbs\n",
      "Filled 257 missing values in Best3BenchKg using Best3BenchLbs\n",
      "Filled 261 missing values in Best3DeadliftKg using Best3DeadliftLbs\n",
      "Filled 263 missing values in Squat1Kg using Squat1Lbs\n",
      "Filled 263 missing values in Squat2Kg using Squat2Lbs\n",
      "Filled 260 missing values in Squat3Kg using Squat3Lbs\n",
      "Filled 264 missing values in Bench1Kg using Bench1Lbs\n",
      "Filled 264 missing values in Bench2Kg using Bench2Lbs\n",
      "Filled 262 missing values in Bench3Kg using Bench3Lbs\n",
      "Filled 262 missing values in Deadlift1Kg using Deadlift1Lbs\n",
      "Filled 261 missing values in Deadlift2Kg using Deadlift2Lbs\n",
      "Filled 261 missing values in Deadlift3Kg using Deadlift3Lbs\n",
      "Filled 245 missing values in TotalKg using TotalLbs\n"
     ]
    }
   ],
   "source": [
    "# Convert any Lbs columns to Kg and fill missing Kg values\n",
    "# Conversion factor: 1 lb = 0.453592 kg\n",
    "\n",
    "# Define the mapping of Lbs columns to their corresponding Kg columns\n",
    "lbs_to_kg_mapping = {\n",
    "    'Best3SquatLbs': 'Best3SquatKg',\n",
    "    'Best3BenchLbs': 'Best3BenchKg',\n",
    "    'Best3DeadliftLbs': 'Best3DeadliftKg',\n",
    "    'Squat1Lbs': 'Squat1Kg',\n",
    "    'Squat2Lbs': 'Squat2Kg',\n",
    "    'Squat3Lbs': 'Squat3Kg',\n",
    "    'Bench1Lbs': 'Bench1Kg',\n",
    "    'Bench2Lbs': 'Bench2Kg',\n",
    "    'Bench3Lbs': 'Bench3Kg',\n",
    "    'Deadlift1Lbs': 'Deadlift1Kg',\n",
    "    'Deadlift2Lbs': 'Deadlift2Kg',\n",
    "    'Deadlift3Lbs': 'Deadlift3Kg',\n",
    "    'TotalLbs': 'TotalKg'\n",
    "}\n",
    "\n",
    "# Convert and fill missing values\n",
    "for lbs_col, kg_col in lbs_to_kg_mapping.items():\n",
    "    if lbs_col in df_entries_all.columns:\n",
    "        # Convert lbs to kg (multiply by 0.453592)\n",
    "        converted_values = df_entries_all[lbs_col] * 0.453592\n",
    "        \n",
    "        # Fill missing Kg values with converted Lbs values (only where Kg is missing and Lbs exists)\n",
    "        mask = df_entries_all[kg_col].isna() & df_entries_all[lbs_col].notna()\n",
    "        df_entries_all.loc[mask, kg_col] = converted_values[mask]\n",
    "        \n",
    "        print(f\"Filled {mask.sum()} missing values in {kg_col} using {lbs_col}\")\n",
    "# Drop all Lbs columns after conversion\n",
    "lbs_columns_to_drop = [col for col in df_entries_all.columns if col.endswith('Lbs')]\n",
    "df_entries_all = df_entries_all.drop(columns=lbs_columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1326911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created LifterID using columns: ['Name', 'Sex', 'BirthYear']\n",
      "   LifterID               Name Sex  BirthYear\n",
      "0    133140      Wei-Ling Chen   F     1982.0\n",
      "1    133140      Wei-Ling Chen   F     1982.0\n",
      "2     99408  Natalia Krikunova   F     1981.0\n",
      "3     99408  Natalia Krikunova   F     1981.0\n",
      "4    104691       Oxana Sirant   F     1979.0\n",
      "5    104691       Oxana Sirant   F     1979.0\n",
      "6      9948        Aneta Rutka   F     1982.0\n",
      "7      9948        Aneta Rutka   F     1982.0\n",
      "8     21493  Bénédicte LePanse   F     1978.0\n",
      "9     21493  Bénédicte LePanse   F     1978.0\n"
     ]
    }
   ],
   "source": [
    "# Create a surrogate LifterID so we can build per-lifter competition histories\n",
    "# We use a combination of fairly stable identity columns.\n",
    "\n",
    "id_cols = [col for col in ['Name', 'Sex', 'BirthYear'] if col in df_entries_all.columns]\n",
    "\n",
    "if id_cols:\n",
    "    # Build a string key from available identity columns\n",
    "    lifter_key = df_entries_all[id_cols].astype(str).agg('|'.join, axis=1)\n",
    "\n",
    "    # Turn the key into an integer LifterID (stable within this run)\n",
    "    df_entries_all['LifterID'] = lifter_key.astype('category').cat.codes\n",
    "\n",
    "    print(\"Created LifterID using columns:\", id_cols)\n",
    "    print(df_entries_all[['LifterID'] + id_cols].head(10))\n",
    "else:\n",
    "    raise ValueError(\"No suitable columns found to construct LifterID.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6073aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-lifter competition history created using 'LifterID' and 'Date'.\n",
      "        LifterID       Date  CompIndex\n",
      "378313         0 2002-01-01          1\n",
      "378312         0 2002-05-29          2\n",
      "378351         1 2002-01-01          1\n",
      "378350         1 2002-05-29          2\n",
      "378679         2 2002-01-01          1\n",
      "378678         2 2002-05-29          2\n",
      "5177           3 2002-02-16          1\n",
      "5176           3 2002-09-25          2\n",
      "405711         4 2023-11-13          1\n",
      "405712         4 2023-12-17          2\n"
     ]
    }
   ],
   "source": [
    "# 2.2 Group by LifterID and sort competitions chronologically\n",
    "# This creates a competition history for each lifter.\n",
    "\n",
    "# Sort by lifter and date so each lifter's rows are in chronological order\n",
    "df_entries_all = df_entries_all.sort_values(['LifterID', 'Date'])\n",
    "\n",
    "# Create an explicit competition index per lifter (1 = first recorded meet, 2 = second, ...)\n",
    "df_entries_all['CompIndex'] = df_entries_all.groupby('LifterID').cumcount() + 1\n",
    "\n",
    "print(\"Per-lifter competition history created using 'LifterID' and 'Date'.\")\n",
    "print(df_entries_all[['LifterID', 'Date', 'CompIndex']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0cd2a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows before filtering by competition count: 406540\n",
      "Total rows after keeping lifters with >= 2 competitions: 349887\n",
      "Number of unique lifters remaining: 80612\n",
      "After filtering to SBD events: 294766 rows (dropped 55121)\n",
      "After filtering to Raw equipment: 233466 rows (dropped 61300)\n",
      "After filtering out rows with missing TotalKg: 225149 rows (dropped 8317)\n",
      "Number of unique lifters remaining: 62404\n"
     ]
    }
   ],
   "source": [
    "# 2.3 Filter valid lifters\n",
    "# - Keep only lifters with at least 2 competitions\n",
    "# - Optionally filter by other criteria (e.g., only SBD events, only Raw equipment)\n",
    "\n",
    "# Count number of competitions per lifter\n",
    "comp_counts = df_entries_all.groupby('LifterID')['CompIndex'].max().rename('NumComps')\n",
    "\n",
    "df_entries_all = df_entries_all.merge(comp_counts, on='LifterID', how='left')\n",
    "\n",
    "# Keep only lifters with at least 2 competitions\n",
    "min_comps = 2\n",
    "valid_mask = df_entries_all['NumComps'] >= min_comps\n",
    "\n",
    "print(f\"Total rows before filtering by competition count: {len(df_entries_all)}\")\n",
    "df_entries_all = df_entries_all[valid_mask].copy()\n",
    "print(f\"Total rows after keeping lifters with >= {min_comps} competitions: {len(df_entries_all)}\")\n",
    "print(f\"Number of unique lifters remaining: {df_entries_all['LifterID'].nunique()}\")\n",
    "\n",
    "# Optional: filter to SBD + Raw if those columns exist\n",
    "if 'Event' in df_entries_all.columns:\n",
    "    before = len(df_entries_all)\n",
    "    df_entries_all = df_entries_all[df_entries_all['Event'] == 'SBD']\n",
    "    print(f\"After filtering to SBD events: {len(df_entries_all)} rows (dropped {before - len(df_entries_all)})\")\n",
    "\n",
    "if 'Equipment' in df_entries_all.columns:\n",
    "    before = len(df_entries_all)\n",
    "    df_entries_all = df_entries_all[df_entries_all['Equipment'] == 'Raw']\n",
    "    print(f\"After filtering to Raw equipment: {len(df_entries_all)} rows (dropped {before - len(df_entries_all)})\")\n",
    "\n",
    "\n",
    "\n",
    "# Filter out rows where TotalKg is null or NaN\n",
    "before = len(df_entries_all)\n",
    "df_entries_all = df_entries_all[df_entries_all['TotalKg'].notna()]\n",
    "print(f\"After filtering out rows with missing TotalKg: {len(df_entries_all)} rows (dropped {before - len(df_entries_all)})\")\n",
    "\n",
    "print(f\"Number of unique lifters remaining: {df_entries_all['LifterID'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d5f3dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the specified columns\n",
    "# NOTE: Date is preserved for time-based features in Step 3\n",
    "columns_to_drop = ['Name', 'Bench1Kg', 'KoreanName', 'ChineseName', 'Bench2Kg', 'Bench3Kg', 'BirthDate', 'Squat1Kg', 'Squat2Kg', 'Squat3Kg', \n",
    "                    'Deadlift1Kg', 'Deadlift2Kg', 'Deadlift3Kg', 'Country', 'BirthYear', 'BodyweightKg', 'Place', 'Event', 'Equipment',\n",
    "                    'Deadlift4Kg', 'Bench4Kg', 'Squat4Kg', 'Team', 'MeetID', 'State', 'AgeRange', 'DeadliftEquipment','Tested', 'CyrillicName', 'MeetCountry', 'MeetState', 'MeetTown', 'MeetName', 'Federation']\n",
    "df_cleaned = df_entries_all.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1789f5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL CLEANED AND FILTERED DATAFRAME:\n",
      "============================================================\n",
      "\n",
      "Cleaned DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 225149 entries, 20 to 406526\n",
      "Data columns (total 12 columns):\n",
      " #   Column           Non-Null Count   Dtype         \n",
      "---  ------           --------------   -----         \n",
      " 0   WeightClassKg    221382 non-null  object        \n",
      " 1   Sex              225149 non-null  object        \n",
      " 2   Division         225149 non-null  object        \n",
      " 3   Best3SquatKg     225138 non-null  float64       \n",
      " 4   Best3BenchKg     225131 non-null  float64       \n",
      " 5   Best3DeadliftKg  225141 non-null  float64       \n",
      " 6   TotalKg          225149 non-null  float64       \n",
      " 7   Age              221083 non-null  float64       \n",
      " 8   Date             225149 non-null  datetime64[ns]\n",
      " 9   LifterID         225149 non-null  int32         \n",
      " 10  CompIndex        225149 non-null  int64         \n",
      " 11  NumComps         225149 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(5), int32(1), int64(2), object(3)\n",
      "memory usage: 21.5+ MB\n",
      "None\n",
      "\n",
      "First few rows of cleaned data:\n",
      "   WeightClassKg Sex Division  Best3SquatKg  Best3BenchKg  Best3DeadliftKg  \\\n",
      "20            56   M     MR-C         182.5         105.0            185.0   \n",
      "21            56   M     MR-C         190.0         112.5            197.5   \n",
      "22            56   M     MR-C         196.5         117.5            212.5   \n",
      "31            93   M    MR-Jr         245.0         165.0            257.5   \n",
      "32            93   M     MR-O         272.5         155.0            280.0   \n",
      "34           105   M     MR-O         287.5         195.0            275.0   \n",
      "36          67.5   F    FR-HS         133.8          61.2            154.2   \n",
      "37           125   M    MR-Jr         260.0         170.0            260.0   \n",
      "38           125   M    MR-Jr         300.0         182.5            265.0   \n",
      "39           125   M    MR-Jr         285.0         185.0            270.0   \n",
      "\n",
      "    TotalKg   Age       Date  LifterID  CompIndex  NumComps  \n",
      "20    472.5  18.0 2024-11-09         9          1         3  \n",
      "21    500.0  19.0 2025-02-08         9          2         3  \n",
      "22    526.5  19.0 2025-04-03         9          3         3  \n",
      "31    667.5  22.0 2015-03-21        13          1         4  \n",
      "32    707.5  23.0 2016-06-04        13          2         4  \n",
      "34    757.5  24.0 2017-04-08        13          4         4  \n",
      "36    349.2  17.0 2024-05-18        14          2         2  \n",
      "37    690.0  23.0 2023-09-09        15          1         4  \n",
      "38    747.5  24.0 2024-03-09        15          2         4  \n",
      "39    740.0  24.0 2024-10-05        15          3         4  \n"
     ]
    }
   ],
   "source": [
    "# Display the cleaned and filtered dataframe\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL CLEANED AND FILTERED DATAFRAME:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nCleaned DataFrame Info:\")\n",
    "print(df_cleaned.info())\n",
    "\n",
    "print(\"\\nFirst few rows of cleaned data:\")\n",
    "print(df_cleaned.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "703fdbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction functions defined.\n",
      "Ready to create training examples...\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Create Training Examples (Sliding Window Approach)\n",
    "# For each lifter with N competitions, create N-1 training examples\n",
    "# Each example uses previous competitions to predict the next competition's lifts\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def extract_historical_features(previous_comps):\n",
    "    \"\"\"\n",
    "    Extract historical performance features from previous competitions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    previous_comps : DataFrame\n",
    "        DataFrame containing all previous competitions for a lifter\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary of historical features\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Previous lifts (last competition)\n",
    "    if len(previous_comps) > 0:\n",
    "        last_comp = previous_comps.iloc[-1]\n",
    "        features['PrevBest3SquatKg'] = last_comp.get('Best3SquatKg', np.nan)\n",
    "        features['PrevBest3BenchKg'] = last_comp.get('Best3BenchKg', np.nan)\n",
    "        features['PrevBest3DeadliftKg'] = last_comp.get('Best3DeadliftKg', np.nan)\n",
    "        features['PrevTotalKg'] = last_comp.get('TotalKg', np.nan)\n",
    "    else:\n",
    "        features['PrevBest3SquatKg'] = np.nan\n",
    "        features['PrevBest3BenchKg'] = np.nan\n",
    "        features['PrevBest3DeadliftKg'] = np.nan\n",
    "        features['PrevTotalKg'] = np.nan\n",
    "    \n",
    "    # All-time PRs from previous competitions\n",
    "    valid_squat = previous_comps['Best3SquatKg'].dropna()\n",
    "    valid_bench = previous_comps['Best3BenchKg'].dropna()\n",
    "    valid_deadlift = previous_comps['Best3DeadliftKg'].dropna()\n",
    "    valid_total = previous_comps['TotalKg'].dropna()\n",
    "    \n",
    "    features['PRBest3SquatKg'] = valid_squat.max() if len(valid_squat) > 0 else np.nan\n",
    "    features['PRBest3BenchKg'] = valid_bench.max() if len(valid_bench) > 0 else np.nan\n",
    "    features['PRBest3DeadliftKg'] = valid_deadlift.max() if len(valid_deadlift) > 0 else np.nan\n",
    "    features['PRTotalKg'] = valid_total.max() if len(valid_total) > 0 else np.nan\n",
    "    \n",
    "    # Averages of last 3 competitions (or all if fewer than 3)\n",
    "    n_recent = min(3, len(previous_comps))\n",
    "    if n_recent > 0:\n",
    "        recent_comps = previous_comps.tail(n_recent)\n",
    "        features['AvgBest3SquatKg_Last3'] = recent_comps['Best3SquatKg'].mean()\n",
    "        features['AvgBest3BenchKg_Last3'] = recent_comps['Best3BenchKg'].mean()\n",
    "        features['AvgBest3DeadliftKg_Last3'] = recent_comps['Best3DeadliftKg'].mean()\n",
    "        features['AvgTotalKg_Last3'] = recent_comps['TotalKg'].mean()\n",
    "    else:\n",
    "        features['AvgBest3SquatKg_Last3'] = np.nan\n",
    "        features['AvgBest3BenchKg_Last3'] = np.nan\n",
    "        features['AvgBest3DeadliftKg_Last3'] = np.nan\n",
    "        features['AvgTotalKg_Last3'] = np.nan\n",
    "    \n",
    "    # Consistency (standard deviation) across all previous competitions\n",
    "    features['StdBest3SquatKg'] = valid_squat.std() if len(valid_squat) > 1 else np.nan\n",
    "    features['StdBest3BenchKg'] = valid_bench.std() if len(valid_bench) > 1 else np.nan\n",
    "    features['StdBest3DeadliftKg'] = valid_deadlift.std() if len(valid_deadlift) > 1 else np.nan\n",
    "    features['StdTotalKg'] = valid_total.std() if len(valid_total) > 1 else np.nan\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_time_features(current_comp, previous_comps, dates):\n",
    "    \"\"\"\n",
    "    Extract time-based features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    current_comp : Series\n",
    "        Current competition row\n",
    "    previous_comps : DataFrame\n",
    "        Previous competitions\n",
    "    dates : Series\n",
    "        Dates for all competitions (current + previous)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary of time-based features\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Competition count (which competition number this is)\n",
    "    features['CompIndex'] = current_comp.get('CompIndex', np.nan)\n",
    "    \n",
    "    # Age at current competition\n",
    "    features['Age'] = current_comp.get('Age', np.nan)\n",
    "    \n",
    "    # Days since last competition\n",
    "    if len(previous_comps) > 0 and 'Date' in current_comp and pd.notna(current_comp['Date']):\n",
    "        last_date = previous_comps.iloc[-1].get('Date')\n",
    "        if pd.notna(last_date):\n",
    "            days_diff = (current_comp['Date'] - last_date).days\n",
    "            features['DaysSinceLastComp'] = days_diff\n",
    "        else:\n",
    "            features['DaysSinceLastComp'] = np.nan\n",
    "    else:\n",
    "        features['DaysSinceLastComp'] = np.nan\n",
    "    \n",
    "    # Average days between competitions (for this lifter)\n",
    "    if len(previous_comps) > 0 and 'Date' in previous_comps.columns:\n",
    "        valid_dates = previous_comps['Date'].dropna()\n",
    "        if len(valid_dates) > 1:\n",
    "            date_diffs = valid_dates.diff().dropna()\n",
    "            features['AvgDaysBetweenComps'] = date_diffs.dt.days.mean() if len(date_diffs) > 0 else np.nan\n",
    "        else:\n",
    "            features['AvgDaysBetweenComps'] = np.nan\n",
    "    else:\n",
    "        features['AvgDaysBetweenComps'] = np.nan\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_context_features(current_comp):\n",
    "    \"\"\"\n",
    "    Extract context features from current competition.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    current_comp : Series\n",
    "        Current competition row\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary of context features\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Weight class (keep as string for now, can encode later)\n",
    "    features['WeightClassKg'] = current_comp.get('WeightClassKg', np.nan)\n",
    "    \n",
    "    # Sex (M/F)\n",
    "    features['Sex'] = current_comp.get('Sex', np.nan)\n",
    "    \n",
    "    # Division (Open, Junior, etc.)\n",
    "    features['Division'] = current_comp.get('Division', np.nan)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_trend_features(all_previous_comps):\n",
    "    \"\"\"\n",
    "    Extract trend/improvement features from all previous competitions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    all_previous_comps : DataFrame\n",
    "        All previous competitions sorted by CompIndex\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary of trend features\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    if len(all_previous_comps) == 0:\n",
    "        # No previous competitions\n",
    "        features['SquatImprovementRate'] = np.nan\n",
    "        features['BenchImprovementRate'] = np.nan\n",
    "        features['DeadliftImprovementRate'] = np.nan\n",
    "        features['SquatImprovementDirection'] = 0\n",
    "        features['BenchImprovementDirection'] = 0\n",
    "        features['DeadliftImprovementDirection'] = 0\n",
    "        features['CompsSincePRSquat'] = np.nan\n",
    "        features['CompsSincePRBench'] = np.nan\n",
    "        features['CompsSincePRDeadlift'] = np.nan\n",
    "        return features\n",
    "    \n",
    "    # Improvement rate (slope) - change per competition\n",
    "    valid_comps = all_previous_comps.dropna(subset=['Best3SquatKg', 'Best3BenchKg', 'Best3DeadliftKg'], how='all')\n",
    "    \n",
    "    if len(valid_comps) >= 2:\n",
    "        # Calculate slope using linear regression (simple: (last - first) / (n_comps - 1))\n",
    "        comp_indices = valid_comps['CompIndex'].values\n",
    "        squat_values = valid_comps['Best3SquatKg'].values\n",
    "        bench_values = valid_comps['Best3BenchKg'].values\n",
    "        deadlift_values = valid_comps['Best3DeadliftKg'].values\n",
    "        \n",
    "        # Squat improvement rate\n",
    "        if len(squat_values[~np.isnan(squat_values)]) >= 2:\n",
    "            valid_squat = ~np.isnan(squat_values)\n",
    "            if valid_squat.sum() >= 2:\n",
    "                valid_indices = comp_indices[valid_squat]\n",
    "                valid_squat_vals = squat_values[valid_squat]\n",
    "                if len(valid_indices) > 1:\n",
    "                    features['SquatImprovementRate'] = (valid_squat_vals[-1] - valid_squat_vals[0]) / (valid_indices[-1] - valid_indices[0])\n",
    "                else:\n",
    "                    features['SquatImprovementRate'] = np.nan\n",
    "            else:\n",
    "                features['SquatImprovementRate'] = np.nan\n",
    "        else:\n",
    "            features['SquatImprovementRate'] = np.nan\n",
    "        \n",
    "        # Bench improvement rate\n",
    "        if len(bench_values[~np.isnan(bench_values)]) >= 2:\n",
    "            valid_bench = ~np.isnan(bench_values)\n",
    "            if valid_bench.sum() >= 2:\n",
    "                valid_indices = comp_indices[valid_bench]\n",
    "                valid_bench_vals = bench_values[valid_bench]\n",
    "                if len(valid_indices) > 1:\n",
    "                    features['BenchImprovementRate'] = (valid_bench_vals[-1] - valid_bench_vals[0]) / (valid_indices[-1] - valid_indices[0])\n",
    "                else:\n",
    "                    features['BenchImprovementRate'] = np.nan\n",
    "            else:\n",
    "                features['BenchImprovementRate'] = np.nan\n",
    "        else:\n",
    "            features['BenchImprovementRate'] = np.nan\n",
    "        \n",
    "        # Deadlift improvement rate\n",
    "        if len(deadlift_values[~np.isnan(deadlift_values)]) >= 2:\n",
    "            valid_deadlift = ~np.isnan(deadlift_values)\n",
    "            if valid_deadlift.sum() >= 2:\n",
    "                valid_indices = comp_indices[valid_deadlift]\n",
    "                valid_deadlift_vals = deadlift_values[valid_deadlift]\n",
    "                if len(valid_indices) > 1:\n",
    "                    features['DeadliftImprovementRate'] = (valid_deadlift_vals[-1] - valid_deadlift_vals[0]) / (valid_indices[-1] - valid_indices[0])\n",
    "                else:\n",
    "                    features['DeadliftImprovementRate'] = np.nan\n",
    "            else:\n",
    "                features['DeadliftImprovementRate'] = np.nan\n",
    "        else:\n",
    "            features['DeadliftImprovementRate'] = np.nan\n",
    "        \n",
    "        # Improvement direction (comparing last 2 competitions)\n",
    "        if len(valid_comps) >= 2:\n",
    "            last_two = valid_comps.tail(2)\n",
    "            last_squat = last_two.iloc[-1]['Best3SquatKg']\n",
    "            prev_squat = last_two.iloc[-2]['Best3SquatKg']\n",
    "            if pd.notna(last_squat) and pd.notna(prev_squat):\n",
    "                features['SquatImprovementDirection'] = 1 if last_squat > prev_squat else (-1 if last_squat < prev_squat else 0)\n",
    "            else:\n",
    "                features['SquatImprovementDirection'] = 0\n",
    "            \n",
    "            last_bench = last_two.iloc[-1]['Best3BenchKg']\n",
    "            prev_bench = last_two.iloc[-2]['Best3BenchKg']\n",
    "            if pd.notna(last_bench) and pd.notna(prev_bench):\n",
    "                features['BenchImprovementDirection'] = 1 if last_bench > prev_bench else (-1 if last_bench < prev_bench else 0)\n",
    "            else:\n",
    "                features['BenchImprovementDirection'] = 0\n",
    "            \n",
    "            last_deadlift = last_two.iloc[-1]['Best3DeadliftKg']\n",
    "            prev_deadlift = last_two.iloc[-2]['Best3DeadliftKg']\n",
    "            if pd.notna(last_deadlift) and pd.notna(prev_deadlift):\n",
    "                features['DeadliftImprovementDirection'] = 1 if last_deadlift > prev_deadlift else (-1 if last_deadlift < prev_deadlift else 0)\n",
    "            else:\n",
    "                features['DeadliftImprovementDirection'] = 0\n",
    "        else:\n",
    "            features['SquatImprovementDirection'] = 0\n",
    "            features['BenchImprovementDirection'] = 0\n",
    "            features['DeadliftImprovementDirection'] = 0\n",
    "        \n",
    "        # Comps since PR (number of competitions since achieving personal record)\n",
    "        pr_squat = all_previous_comps['Best3SquatKg'].max()\n",
    "        pr_bench = all_previous_comps['Best3BenchKg'].max()\n",
    "        pr_deadlift = all_previous_comps['Best3DeadliftKg'].max()\n",
    "        \n",
    "        if pd.notna(pr_squat):\n",
    "            pr_squat_comps = all_previous_comps[all_previous_comps['Best3SquatKg'] == pr_squat]\n",
    "            if len(pr_squat_comps) > 0:\n",
    "                last_pr_comp = pr_squat_comps['CompIndex'].max()\n",
    "                last_comp = all_previous_comps['CompIndex'].max()\n",
    "                features['CompsSincePRSquat'] = last_comp - last_pr_comp\n",
    "            else:\n",
    "                features['CompsSincePRSquat'] = np.nan\n",
    "        else:\n",
    "            features['CompsSincePRSquat'] = np.nan\n",
    "        \n",
    "        if pd.notna(pr_bench):\n",
    "            pr_bench_comps = all_previous_comps[all_previous_comps['Best3BenchKg'] == pr_bench]\n",
    "            if len(pr_bench_comps) > 0:\n",
    "                last_pr_comp = pr_bench_comps['CompIndex'].max()\n",
    "                last_comp = all_previous_comps['CompIndex'].max()\n",
    "                features['CompsSincePRBench'] = last_comp - last_pr_comp\n",
    "            else:\n",
    "                features['CompsSincePRBench'] = np.nan\n",
    "        else:\n",
    "            features['CompsSincePRBench'] = np.nan\n",
    "        \n",
    "        if pd.notna(pr_deadlift):\n",
    "            pr_deadlift_comps = all_previous_comps[all_previous_comps['Best3DeadliftKg'] == pr_deadlift]\n",
    "            if len(pr_deadlift_comps) > 0:\n",
    "                last_pr_comp = pr_deadlift_comps['CompIndex'].max()\n",
    "                last_comp = all_previous_comps['CompIndex'].max()\n",
    "                features['CompsSincePRDeadlift'] = last_comp - last_pr_comp\n",
    "            else:\n",
    "                features['CompsSincePRDeadlift'] = np.nan\n",
    "        else:\n",
    "            features['CompsSincePRDeadlift'] = np.nan\n",
    "    else:\n",
    "        # Not enough competitions for trend analysis\n",
    "        features['SquatImprovementRate'] = np.nan\n",
    "        features['BenchImprovementRate'] = np.nan\n",
    "        features['DeadliftImprovementRate'] = np.nan\n",
    "        features['SquatImprovementDirection'] = 0\n",
    "        features['BenchImprovementDirection'] = 0\n",
    "        features['DeadliftImprovementDirection'] = 0\n",
    "        features['CompsSincePRSquat'] = np.nan\n",
    "        features['CompsSincePRBench'] = np.nan\n",
    "        features['CompsSincePRDeadlift'] = np.nan\n",
    "    \n",
    "    return features\n",
    "\n",
    "def create_training_examples(df):\n",
    "    \"\"\"\n",
    "    Create training examples using sliding window approach.\n",
    "    \n",
    "    For each lifter with N competitions, creates N-1 training examples:\n",
    "    - Example 1: Use comp 1 → predict comp 2\n",
    "    - Example 2: Use comps 1-2 → predict comp 3\n",
    "    - Example 3: Use comps 1-3 → predict comp 4\n",
    "    - etc.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        DataFrame with per-lifter competition histories (sorted by LifterID and Date)\n",
    "        Must have columns: LifterID, CompIndex, Date, and lift columns\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    df_training : DataFrame\n",
    "        DataFrame with features (X) and targets (y) for training\n",
    "    \"\"\"\n",
    "    training_examples = []\n",
    "    \n",
    "    # Group by LifterID\n",
    "    for lifter_id, lifter_group in df.groupby('LifterID'):\n",
    "        # Sort by CompIndex to ensure chronological order\n",
    "        lifter_group = lifter_group.sort_values('CompIndex').reset_index(drop=True)\n",
    "        \n",
    "        # For each competition from 2nd onwards (we need at least 1 previous comp)\n",
    "        for idx in range(1, len(lifter_group)):\n",
    "            current_comp = lifter_group.iloc[idx]\n",
    "            previous_comps = lifter_group.iloc[:idx]  # All competitions before current\n",
    "            \n",
    "            # Extract all features\n",
    "            example = {}\n",
    "            \n",
    "            # Metadata\n",
    "            example['LifterID'] = lifter_id\n",
    "            example['CompIndex'] = current_comp['CompIndex']\n",
    "            \n",
    "            # Historical features\n",
    "            hist_features = extract_historical_features(previous_comps)\n",
    "            example.update(hist_features)\n",
    "            \n",
    "            # Time-based features\n",
    "            time_features = extract_time_features(current_comp, previous_comps, lifter_group['Date'])\n",
    "            example.update(time_features)\n",
    "            \n",
    "            # Context features\n",
    "            context_features = extract_context_features(current_comp)\n",
    "            example.update(context_features)\n",
    "            \n",
    "            # Trend features\n",
    "            trend_features = extract_trend_features(previous_comps)\n",
    "            example.update(trend_features)\n",
    "            \n",
    "            # Target variables (what we're trying to predict)\n",
    "            example['NextBest3SquatKg'] = current_comp.get('Best3SquatKg', np.nan)\n",
    "            example['NextBest3BenchKg'] = current_comp.get('Best3BenchKg', np.nan)\n",
    "            example['NextBest3DeadliftKg'] = current_comp.get('Best3DeadliftKg', np.nan)\n",
    "            \n",
    "            training_examples.append(example)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df_training = pd.DataFrame(training_examples)\n",
    "    \n",
    "    return df_training\n",
    "\n",
    "print(\"Feature extraction functions defined.\")\n",
    "print(\"Ready to create training examples...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3e2bc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples created: 169681\n",
      "Training data shape: (169681, 36)\n",
      "    LifterID  CompIndex  PrevBest3SquatKg  PrevBest3BenchKg  \\\n",
      "21         9          2             182.5             105.0   \n",
      "22         9          3             190.0             112.5   \n",
      "32        13          2             245.0             165.0   \n",
      "34        13          4             272.5             155.0   \n",
      "36        14          2               NaN               NaN   \n",
      "\n",
      "    PrevBest3DeadliftKg  PrevTotalKg  PRBest3SquatKg  PRBest3BenchKg  \\\n",
      "21                185.0        472.5           182.5           105.0   \n",
      "22                197.5        500.0           190.0           112.5   \n",
      "32                257.5        667.5           245.0           165.0   \n",
      "34                280.0        707.5           272.5           165.0   \n",
      "36                  NaN          NaN           287.5           195.0   \n",
      "\n",
      "    PRBest3DeadliftKg  PRTotalKg  ...  DeadliftImprovementRate  \\\n",
      "21              185.0      472.5  ...                      NaN   \n",
      "22              197.5      500.0  ...                12.500000   \n",
      "32              257.5      667.5  ...                      NaN   \n",
      "34              280.0      707.5  ...                22.500000   \n",
      "36              280.0      757.5  ...                 4.642857   \n",
      "\n",
      "    SquatImprovementDirection  BenchImprovementDirection  \\\n",
      "21                          0                          0   \n",
      "22                          1                          1   \n",
      "32                          0                          0   \n",
      "34                          1                         -1   \n",
      "36                          0                          0   \n",
      "\n",
      "    DeadliftImprovementDirection  CompsSincePRSquat  CompsSincePRBench  \\\n",
      "21                             0                0.0                0.0   \n",
      "22                             1                0.0                0.0   \n",
      "32                             0                0.0                0.0   \n",
      "34                             1                1.0                2.0   \n",
      "36                             0                NaN                NaN   \n",
      "\n",
      "    CompsSincePRDeadlift  NextBest3SquatKg  NextBest3BenchKg  \\\n",
      "21                   0.0             190.0             112.5   \n",
      "22                   0.0             196.5             117.5   \n",
      "32                   0.0             272.5             155.0   \n",
      "34                   1.0             287.5             195.0   \n",
      "36                   NaN             133.8              61.2   \n",
      "\n",
      "    NextBest3DeadliftKg  \n",
      "21                197.5  \n",
      "22                212.5  \n",
      "32                280.0  \n",
      "34                275.0  \n",
      "36                154.2  \n",
      "\n",
      "[5 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 3 (FAST + FULL HISTORY REGRESSION)\n",
    "import numpy as np\n",
    "\n",
    "# Sort once\n",
    "df = df_cleaned.sort_values(['LifterID', 'CompIndex']).copy()\n",
    "g = df.groupby('LifterID', sort=False)\n",
    "\n",
    "# ---- Previous competition features\n",
    "df['PrevBest3SquatKg'] = g['Best3SquatKg'].shift(1)\n",
    "df['PrevBest3BenchKg'] = g['Best3BenchKg'].shift(1)\n",
    "df['PrevBest3DeadliftKg'] = g['Best3DeadliftKg'].shift(1)\n",
    "df['PrevTotalKg'] = g['TotalKg'].shift(1)\n",
    "\n",
    "# ---- PRs (best up to previous comp)\n",
    "df['PRBest3SquatKg'] = g['Best3SquatKg'].cummax().shift(1)\n",
    "df['PRBest3BenchKg'] = g['Best3BenchKg'].cummax().shift(1)\n",
    "df['PRBest3DeadliftKg'] = g['Best3DeadliftKg'].cummax().shift(1)\n",
    "df['PRTotalKg'] = g['TotalKg'].cummax().shift(1)\n",
    "\n",
    "# ---- Rolling averages (last 3 previous comps)\n",
    "for col in ['Best3SquatKg', 'Best3BenchKg', 'Best3DeadliftKg', 'TotalKg']:\n",
    "    df[f'Avg{col}_Last3'] = (\n",
    "        g[col].rolling(3, min_periods=1).mean().shift(1)\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "# ---- Std across all previous comps\n",
    "for col in ['Best3SquatKg', 'Best3BenchKg', 'Best3DeadliftKg', 'TotalKg']:\n",
    "    df[f'Std{col}'] = (\n",
    "        g[col].expanding().std().shift(1)\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "# ---- Time-based features\n",
    "df['DaysSinceLastComp'] = g['Date'].diff().dt.days\n",
    "df['AvgDaysBetweenComps'] = (\n",
    "    g['Date'].diff().dt.days\n",
    "    .groupby(df['LifterID']).expanding().mean().shift(1)\n",
    "    .reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "# ---- FULL HISTORY REGRESSION SLOPE (vectorized)\n",
    "def full_history_slope(df, ycol):\n",
    "    x = df['CompIndex'].astype(float)\n",
    "    y = df[ycol].astype(float)\n",
    "    mask = y.notna()\n",
    "\n",
    "    # cumulative sums over valid points only\n",
    "    x_valid = x.where(mask)\n",
    "    y_valid = y.where(mask)\n",
    "\n",
    "    n = mask.groupby(df['LifterID']).cumsum().shift(1)\n",
    "    sum_x = x_valid.groupby(df['LifterID']).cumsum().shift(1)\n",
    "    sum_y = y_valid.groupby(df['LifterID']).cumsum().shift(1)\n",
    "    sum_x2 = (x_valid * x_valid).groupby(df['LifterID']).cumsum().shift(1)\n",
    "    sum_xy = (x_valid * y_valid).groupby(df['LifterID']).cumsum().shift(1)\n",
    "\n",
    "    denom = (n * sum_x2) - (sum_x * sum_x)\n",
    "    slope = (n * sum_xy - sum_x * sum_y) / denom\n",
    "\n",
    "    # need at least 2 valid points and nonzero denom\n",
    "    slope = slope.where((n >= 2) & (denom != 0))\n",
    "    return slope\n",
    "\n",
    "df['SquatImprovementRate'] = full_history_slope(df, 'Best3SquatKg')\n",
    "df['BenchImprovementRate'] = full_history_slope(df, 'Best3BenchKg')\n",
    "df['DeadliftImprovementRate'] = full_history_slope(df, 'Best3DeadliftKg')\n",
    "\n",
    "# ---- Improvement direction (same logic: compare last two comps)\n",
    "prev_squat_1 = g['Best3SquatKg'].shift(1)\n",
    "prev_squat_2 = g['Best3SquatKg'].shift(2)\n",
    "prev_bench_1 = g['Best3BenchKg'].shift(1)\n",
    "prev_bench_2 = g['Best3BenchKg'].shift(2)\n",
    "prev_dead_1 = g['Best3DeadliftKg'].shift(1)\n",
    "prev_dead_2 = g['Best3DeadliftKg'].shift(2)\n",
    "\n",
    "df['SquatImprovementDirection'] = np.sign(prev_squat_1 - prev_squat_2).fillna(0).astype(int)\n",
    "df['BenchImprovementDirection'] = np.sign(prev_bench_1 - prev_bench_2).fillna(0).astype(int)\n",
    "df['DeadliftImprovementDirection'] = np.sign(prev_dead_1 - prev_dead_2).fillna(0).astype(int)\n",
    "\n",
    "# ---- Comps since PR (exact)\n",
    "def comps_since_pr(series):\n",
    "    pr = series.groupby(df['LifterID']).cummax()\n",
    "    is_pr = series == pr\n",
    "    last_pr_idx = df['CompIndex'].where(is_pr).groupby(df['LifterID']).ffill()\n",
    "    last_pr_idx_prev = last_pr_idx.groupby(df['LifterID']).shift(1)\n",
    "    return (df['CompIndex'] - 1) - last_pr_idx_prev\n",
    "\n",
    "df['CompsSincePRSquat'] = comps_since_pr(df['Best3SquatKg'])\n",
    "df['CompsSincePRBench'] = comps_since_pr(df['Best3BenchKg'])\n",
    "df['CompsSincePRDeadlift'] = comps_since_pr(df['Best3DeadliftKg'])\n",
    "\n",
    "# ---- Targets (current comp)\n",
    "df['NextBest3SquatKg'] = df['Best3SquatKg']\n",
    "df['NextBest3BenchKg'] = df['Best3BenchKg']\n",
    "df['NextBest3DeadliftKg'] = df['Best3DeadliftKg']\n",
    "\n",
    "# ---- Build training set (drop first comp per lifter)\n",
    "feature_cols = [\n",
    "    'LifterID', 'CompIndex',\n",
    "    'PrevBest3SquatKg', 'PrevBest3BenchKg', 'PrevBest3DeadliftKg', 'PrevTotalKg',\n",
    "    'PRBest3SquatKg', 'PRBest3BenchKg', 'PRBest3DeadliftKg', 'PRTotalKg',\n",
    "    'AvgBest3SquatKg_Last3', 'AvgBest3BenchKg_Last3', 'AvgBest3DeadliftKg_Last3', 'AvgTotalKg_Last3',\n",
    "    'StdBest3SquatKg', 'StdBest3BenchKg', 'StdBest3DeadliftKg', 'StdTotalKg',\n",
    "    'Age', 'DaysSinceLastComp', 'AvgDaysBetweenComps',\n",
    "    'WeightClassKg', 'Sex', 'Division',\n",
    "    'SquatImprovementRate', 'BenchImprovementRate', 'DeadliftImprovementRate',\n",
    "    'SquatImprovementDirection', 'BenchImprovementDirection', 'DeadliftImprovementDirection',\n",
    "    'CompsSincePRSquat', 'CompsSincePRBench', 'CompsSincePRDeadlift',\n",
    "    'NextBest3SquatKg', 'NextBest3BenchKg', 'NextBest3DeadliftKg'\n",
    "]\n",
    "\n",
    "df_training = df.loc[df['CompIndex'] >= 2, feature_cols].copy()\n",
    "\n",
    "print(f\"Training examples created: {len(df_training)}\")\n",
    "print(f\"Training data shape: {df_training.shape}\")\n",
    "print(df_training.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bee834a",
   "metadata": {},
   "source": [
    "## Create Training Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d92d9108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 4: FEATURE ENGINEERING - CATEGORICAL ENCODING\n",
      "============================================================\n",
      "\n",
      "WeightClassKg encoded: 166586/169681 successfully parsed\n",
      "Sex encoded (binary): 169651/169681 successfully encoded\n",
      "Division encoded (label): 169681/169681 successfully encoded\n",
      "  Unique divisions: 176\n",
      "\n",
      "All feature columns are numeric: 31 features\n",
      "\n",
      "Step 4 Complete: Feature encoding done\n",
      "Final feature count: 31\n",
      "Dataset shape: (169681, 36)\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Feature Engineering - Encode Categorical Features\n",
    "# Convert categorical features (WeightClassKg, Sex, Division) to numeric\n",
    "\n",
    "try:\n",
    "    # Check if df_training exists\n",
    "    if 'df_training' not in locals() and 'df_training' not in globals():\n",
    "        raise NameError(\"df_training not found. Please run Step 3 first.\")\n",
    "    \n",
    "    if df_training.empty:\n",
    "        raise ValueError(\"df_training is empty. Cannot encode features.\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"STEP 4: FEATURE ENGINEERING - CATEGORICAL ENCODING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_training_encoded = df_training.copy()\n",
    "    \n",
    "    # 4.1 Encode WeightClassKg - Extract numeric value, handle \"+\" suffix\n",
    "    if 'WeightClassKg' in df_training_encoded.columns:\n",
    "        def parse_weight_class(wc):\n",
    "            \"\"\"Parse weight class string to numeric value.\"\"\"\n",
    "            if pd.isna(wc):\n",
    "                return np.nan\n",
    "            try:\n",
    "                # Handle \"+\" suffix (e.g., \"120+\" -> 120)\n",
    "                wc_str = str(wc).strip()\n",
    "                if wc_str.endswith('+'):\n",
    "                    return float(wc_str[:-1])\n",
    "                # Try to convert to float\n",
    "                return float(wc_str)\n",
    "            except (ValueError, TypeError):\n",
    "                # If parsing fails, return NaN\n",
    "                return np.nan\n",
    "        \n",
    "        df_training_encoded['WeightClassKg_Numeric'] = df_training_encoded['WeightClassKg'].apply(parse_weight_class)\n",
    "        parsed_count = df_training_encoded['WeightClassKg_Numeric'].notna().sum()\n",
    "        print(f\"\\nWeightClassKg encoded: {parsed_count}/{len(df_training_encoded)} successfully parsed\")\n",
    "        \n",
    "        # Drop original WeightClassKg (keep numeric version)\n",
    "        df_training_encoded = df_training_encoded.drop(columns=['WeightClassKg'])\n",
    "        df_training_encoded = df_training_encoded.rename(columns={'WeightClassKg_Numeric': 'WeightClassKg'})\n",
    "    \n",
    "    # 4.2 Encode Sex - Binary encoding (M=1, F=0)\n",
    "    if 'Sex' in df_training_encoded.columns:\n",
    "        sex_mapping = {'M': 1, 'F': 0, 'm': 1, 'f': 0}\n",
    "        df_training_encoded['Sex'] = df_training_encoded['Sex'].map(sex_mapping)\n",
    "        encoded_count = df_training_encoded['Sex'].notna().sum()\n",
    "        print(f\"Sex encoded (binary): {encoded_count}/{len(df_training_encoded)} successfully encoded\")\n",
    "    \n",
    "    # 4.3 Encode Division - Label encoding (since it may have many categories)\n",
    "    if 'Division' in df_training_encoded.columns:\n",
    "        # Get unique divisions\n",
    "        unique_divisions = df_training_encoded['Division'].dropna().unique()\n",
    "        division_mapping = {div: idx for idx, div in enumerate(unique_divisions)}\n",
    "        \n",
    "        # Apply label encoding\n",
    "        df_training_encoded['Division'] = df_training_encoded['Division'].map(division_mapping)\n",
    "        encoded_count = df_training_encoded['Division'].notna().sum()\n",
    "        print(f\"Division encoded (label): {encoded_count}/{len(df_training_encoded)} successfully encoded\")\n",
    "        print(f\"  Unique divisions: {len(unique_divisions)}\")\n",
    "    \n",
    "    # 4.4 Validate all features are numeric (except metadata and targets)\n",
    "    metadata_cols = ['LifterID', 'CompIndex']\n",
    "    target_cols = ['NextBest3SquatKg', 'NextBest3BenchKg', 'NextBest3DeadliftKg']\n",
    "    feature_cols = [col for col in df_training_encoded.columns \n",
    "                    if col not in metadata_cols + target_cols]\n",
    "    \n",
    "    # Check for string values in feature columns\n",
    "    string_features = []\n",
    "    for col in feature_cols:\n",
    "        if df_training_encoded[col].dtype == 'object':\n",
    "            string_features.append(col)\n",
    "    \n",
    "    if string_features:\n",
    "        print(f\"\\nWarning: String features found: {string_features}\")\n",
    "        print(\"These features may need additional encoding.\")\n",
    "    else:\n",
    "        print(f\"\\nAll feature columns are numeric: {len(feature_cols)} features\")\n",
    "    \n",
    "    # Update df_training\n",
    "    df_training = df_training_encoded.copy()\n",
    "    \n",
    "    print(f\"\\nStep 4 Complete: Feature encoding done\")\n",
    "    print(f\"Final feature count: {len(feature_cols)}\")\n",
    "    print(f\"Dataset shape: {df_training.shape}\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in feature encoding: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2979d5d0",
   "metadata": {},
   "source": [
    "## Feature Engineering - Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b13c891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 5: HANDLE MISSING DATA AND EDGE CASES\n",
      "============================================================\n",
      "\n",
      "Missing target values:\n",
      "  NextBest3SquatKg: 6 (0.00%)\n",
      "  NextBest3BenchKg: 7 (0.00%)\n",
      "  NextBest3DeadliftKg: 5 (0.00%)\n",
      "\n",
      "Dropped 0 rows where all targets are missing\n",
      "\n",
      "Data Quality Checks:\n",
      "  ✓ No negative target values found\n",
      "  ✓ No extreme outliers found\n",
      "  Warning: 164 invalid ages (not in 10-100 range) - setting to NaN\n",
      "  Warning: 13 invalid days between competitions (not in 0-3650 range) - setting to NaN\n",
      "\n",
      "Imputing missing feature values:\n",
      "  PrevBest3SquatKg: 6942 missing values imputed with median (167.50)\n",
      "  PrevBest3BenchKg: 6949 missing values imputed with median (105.00)\n",
      "  PrevBest3DeadliftKg: 6941 missing values imputed with median (195.00)\n",
      "  PrevTotalKg: 6936 missing values imputed with median (470.00)\n",
      "  PRBest3SquatKg: 7 missing values imputed with median (167.50)\n",
      "  PRBest3BenchKg: 13 missing values imputed with median (107.50)\n",
      "  PRBest3DeadliftKg: 5 missing values imputed with median (197.50)\n",
      "  AvgBest3SquatKg_Last3: 4 missing values imputed with median (164.17)\n",
      "  AvgBest3BenchKg_Last3: 13 missing values imputed with median (105.00)\n",
      "  AvgBest3DeadliftKg_Last3: 3 missing values imputed with median (192.50)\n",
      "  StdBest3SquatKg: 58555 missing values imputed with median (8.84)\n",
      "  StdBest3BenchKg: 58558 missing values imputed with median (5.00)\n",
      "  StdBest3DeadliftKg: 58554 missing values imputed with median (8.84)\n",
      "  StdTotalKg: 58551 missing values imputed with median (21.21)\n",
      "  Age: 2997 missing values imputed with median (24.00)\n",
      "  DaysSinceLastComp: 6949 missing values imputed with median (152.00)\n",
      "  AvgDaysBetweenComps: 58551 missing values imputed with median (153.67)\n",
      "  Sex: 30 missing values imputed with median (1.00)\n",
      "  SquatImprovementRate: 58556 missing values imputed with median (5.00)\n",
      "  BenchImprovementRate: 58558 missing values imputed with median (2.50)\n",
      "  DeadliftImprovementRate: 58555 missing values imputed with median (4.43)\n",
      "  CompsSincePRSquat: 6939 missing values imputed with median (0.00)\n",
      "  CompsSincePRBench: 6949 missing values imputed with median (0.00)\n",
      "  CompsSincePRDeadlift: 6939 missing values imputed with median (0.00)\n",
      "  WeightClassKg: 3095 missing values imputed with median (83.00)\n",
      "\n",
      "Step 5 Complete:\n",
      "  Initial examples: 169681\n",
      "  Final examples: 169681\n",
      "  Rows dropped: 0\n",
      "  Features imputed: 25\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Handle Missing Data and Edge Cases\n",
    "\n",
    "try:\n",
    "    if 'df_training' not in locals() and 'df_training' not in globals():\n",
    "        raise NameError(\"df_training not found. Please run Step 4 first.\")\n",
    "    \n",
    "    if df_training.empty:\n",
    "        raise ValueError(\"df_training is empty. Cannot process missing data.\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"STEP 5: HANDLE MISSING DATA AND EDGE CASES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    df_training_cleaned = df_training.copy()\n",
    "    initial_count = len(df_training_cleaned)\n",
    "    \n",
    "    # 5.1 Handle Missing Target Values\n",
    "    target_cols = ['NextBest3SquatKg', 'NextBest3BenchKg', 'NextBest3DeadliftKg']\n",
    "    \n",
    "    # Count missing targets\n",
    "    missing_targets = df_training_cleaned[target_cols].isna().sum()\n",
    "    print(f\"\\nMissing target values:\")\n",
    "    for col in target_cols:\n",
    "        print(f\"  {col}: {missing_targets[col]} ({missing_targets[col]/initial_count*100:.2f}%)\")\n",
    "    \n",
    "    # Drop rows where ALL targets are missing (cannot train)\n",
    "    all_targets_missing = df_training_cleaned[target_cols].isna().all(axis=1)\n",
    "    rows_dropped_targets = all_targets_missing.sum()\n",
    "    df_training_cleaned = df_training_cleaned[~all_targets_missing].copy()\n",
    "    print(f\"\\nDropped {rows_dropped_targets} rows where all targets are missing\")\n",
    "    \n",
    "    # 5.2 Data Quality Checks\n",
    "    print(f\"\\nData Quality Checks:\")\n",
    "    \n",
    "    # Check for negative lift values (targets)\n",
    "    negative_targets = 0\n",
    "    for col in target_cols:\n",
    "        negative_mask = df_training_cleaned[col] < 0\n",
    "        negative_count = negative_mask.sum()\n",
    "        if negative_count > 0:\n",
    "            negative_targets += negative_count\n",
    "            print(f\"  Warning: {negative_count} negative values in {col} - setting to NaN\")\n",
    "            df_training_cleaned.loc[negative_mask, col] = np.nan\n",
    "    \n",
    "    if negative_targets == 0:\n",
    "        print(f\"  ✓ No negative target values found\")\n",
    "    \n",
    "    # Check for unrealistic lift values (flag outliers)\n",
    "    # Use world records as reference (rough estimates: Squat ~500kg, Bench ~400kg, Deadlift ~500kg)\n",
    "    world_records = {'NextBest3SquatKg': 500, 'NextBest3BenchKg': 400, 'NextBest3DeadliftKg': 500}\n",
    "    outlier_count = 0\n",
    "    \n",
    "    for col, max_reasonable in world_records.items():\n",
    "        if col in df_training_cleaned.columns:\n",
    "            # Flag values > 3x world record as outliers\n",
    "            outlier_threshold = max_reasonable * 3\n",
    "            outlier_mask = df_training_cleaned[col] > outlier_threshold\n",
    "            outlier_num = outlier_mask.sum()\n",
    "            if outlier_num > 0:\n",
    "                outlier_count += outlier_num\n",
    "                print(f\"  Warning: {outlier_num} outliers in {col} (> {outlier_threshold:.0f}kg) - setting to NaN\")\n",
    "                df_training_cleaned.loc[outlier_mask, col] = np.nan\n",
    "    \n",
    "    if outlier_count == 0:\n",
    "        print(f\"  ✓ No extreme outliers found\")\n",
    "    \n",
    "    # Age sanity checks (10-100 years)\n",
    "    if 'Age' in df_training_cleaned.columns:\n",
    "        age_mask = (df_training_cleaned['Age'] < 10) | (df_training_cleaned['Age'] > 100)\n",
    "        invalid_age_count = age_mask.sum()\n",
    "        if invalid_age_count > 0:\n",
    "            print(f\"  Warning: {invalid_age_count} invalid ages (not in 10-100 range) - setting to NaN\")\n",
    "            df_training_cleaned.loc[age_mask, 'Age'] = np.nan\n",
    "        else:\n",
    "            print(f\"  ✓ All ages in reasonable range (10-100 years)\")\n",
    "    \n",
    "    # Days between competitions sanity check (0-3650 days = 10 years max)\n",
    "    if 'DaysSinceLastComp' in df_training_cleaned.columns:\n",
    "        days_mask = (df_training_cleaned['DaysSinceLastComp'] < 0) | (df_training_cleaned['DaysSinceLastComp'] > 3650)\n",
    "        invalid_days_count = days_mask.sum()\n",
    "        if invalid_days_count > 0:\n",
    "            print(f\"  Warning: {invalid_days_count} invalid days between competitions (not in 0-3650 range) - setting to NaN\")\n",
    "            df_training_cleaned.loc[days_mask, 'DaysSinceLastComp'] = np.nan\n",
    "    \n",
    "    # 5.3 Impute Missing Feature Values\n",
    "    print(f\"\\nImputing missing feature values:\")\n",
    "    \n",
    "    metadata_cols = ['LifterID', 'CompIndex']\n",
    "    feature_cols = [col for col in df_training_cleaned.columns \n",
    "                    if col not in metadata_cols + target_cols]\n",
    "    \n",
    "    imputation_counts = {}\n",
    "    \n",
    "    for col in feature_cols:\n",
    "        missing_count = df_training_cleaned[col].isna().sum()\n",
    "        if missing_count > 0:\n",
    "            if df_training_cleaned[col].dtype in ['int64', 'float64']:\n",
    "                # Numeric: use median (more robust than mean)\n",
    "                imputation_value = df_training_cleaned[col].median()\n",
    "                df_training_cleaned[col].fillna(imputation_value, inplace=True)\n",
    "                imputation_counts[col] = missing_count\n",
    "                print(f\"  {col}: {missing_count} missing values imputed with median ({imputation_value:.2f})\")\n",
    "            else:\n",
    "                # Categorical: use mode\n",
    "                mode_value = df_training_cleaned[col].mode()\n",
    "                if len(mode_value) > 0:\n",
    "                    imputation_value = mode_value[0]\n",
    "                    df_training_cleaned[col].fillna(imputation_value, inplace=True)\n",
    "                    imputation_counts[col] = missing_count\n",
    "                    print(f\"  {col}: {missing_count} missing values imputed with mode ({imputation_value})\")\n",
    "    \n",
    "    if len(imputation_counts) == 0:\n",
    "        print(f\"  ✓ No missing feature values to impute\")\n",
    "    \n",
    "    # Update df_training\n",
    "    df_training = df_training_cleaned.copy()\n",
    "    \n",
    "    print(f\"\\nStep 5 Complete:\")\n",
    "    print(f\"  Initial examples: {initial_count}\")\n",
    "    print(f\"  Final examples: {len(df_training)}\")\n",
    "    print(f\"  Rows dropped: {initial_count - len(df_training)}\")\n",
    "    print(f\"  Features imputed: {len(imputation_counts)}\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in missing data handling: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ee44d6",
   "metadata": {},
   "source": [
    "## Time series split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efe3f64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 6: TIME-BASED DATA SPLIT\n",
      "============================================================\n",
      "\n",
      "Dates added to training data: 169681/169681 examples have dates\n",
      "\n",
      "Split Statistics:\n",
      "  Total examples: 169681\n",
      "  Training set: 118776 (70.0%)\n",
      "  Validation set: 25452 (15.0%)\n",
      "  Test set: 25453 (15.0%)\n",
      "\n",
      "Date Range Validation:\n",
      "  Training: 1974-11-09 00:00:00 to 2023-08-27 00:00:00\n",
      "  Validation: 2023-08-27 00:00:00 to 2024-09-21 00:00:00\n",
      "  Test: 2024-09-21 00:00:00 to 2025-12-24 00:00:00\n",
      "  ⚠ Warning: Potential data leakage - train max date >= val min date\n",
      "  ⚠ Warning: Potential data leakage - val max date >= test min date\n",
      "\n",
      "Step 6 Complete: Data splits created successfully\n",
      "  ✓ Training examples: 118776\n",
      "  ✓ Validation examples: 25452\n",
      "  ✓ Test examples: 25453\n",
      "\n",
      "Sample from training set:\n",
      "   LifterID  CompIndex  PrevBest3SquatKg  PrevBest3BenchKg  \\\n",
      "0     38181          2            408.23            263.08   \n",
      "1     11399          2            167.50            105.00   \n",
      "2    107800          3            188.24            117.93   \n",
      "\n",
      "   PrevBest3DeadliftKg  PrevTotalKg  PRBest3SquatKg  PRBest3BenchKg  \\\n",
      "0               362.87      1034.18          408.23          263.08   \n",
      "1               195.00       470.00          190.00          120.00   \n",
      "2               226.80       532.97          188.24          117.93   \n",
      "\n",
      "   PRBest3DeadliftKg  PRTotalKg  ...  BenchImprovementDirection  \\\n",
      "0             362.87    1034.18  ...                          0   \n",
      "1             215.00     522.50  ...                          0   \n",
      "2             226.80     532.97  ...                          0   \n",
      "\n",
      "   DeadliftImprovementDirection  CompsSincePRSquat  CompsSincePRBench  \\\n",
      "0                             0                0.0                0.0   \n",
      "1                             0                0.0                0.0   \n",
      "2                             0                1.0                1.0   \n",
      "\n",
      "   CompsSincePRDeadlift  NextBest3SquatKg  NextBest3BenchKg  \\\n",
      "0                   0.0             410.0             255.0   \n",
      "1                   0.0             312.5             167.5   \n",
      "2                   1.0             195.0             127.5   \n",
      "\n",
      "   NextBest3DeadliftKg  WeightClassKg       Date  \n",
      "0                377.5          110.0 1974-11-09  \n",
      "1                295.0           90.0 1974-11-09  \n",
      "2                230.0           56.0 1977-11-03  \n",
      "\n",
      "[3 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Split Your Data - Time-Based Split\n",
    "# Split by date: older competitions → train, middle → validation, newest → test\n",
    "\n",
    "try:\n",
    "    if 'df_training' not in locals() and 'df_training' not in globals():\n",
    "        raise NameError(\"df_training not found. Please run Step 5 first.\")\n",
    "    \n",
    "    if df_training.empty:\n",
    "        raise ValueError(\"df_training is empty. Cannot split data.\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"STEP 6: TIME-BASED DATA SPLIT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Add Date column to df_training for splitting\n",
    "    # We need to merge back with df_cleaned to get dates\n",
    "    try:\n",
    "        if 'df_cleaned' in locals() or 'df_cleaned' in globals():\n",
    "            # Merge Date from df_cleaned using LifterID and CompIndex\n",
    "            # Create a mapping from (LifterID, CompIndex) to Date\n",
    "            date_mapping = df_cleaned.set_index(['LifterID', 'CompIndex'])['Date'].to_dict()\n",
    "            \n",
    "            # Add Date to df_training\n",
    "            df_training['Date'] = df_training.apply(\n",
    "                lambda row: date_mapping.get((row['LifterID'], row['CompIndex']), pd.NaT),\n",
    "                axis=1\n",
    "            )\n",
    "            \n",
    "            # Check if we have dates\n",
    "            dates_available = df_training['Date'].notna().sum()\n",
    "            print(f\"\\nDates added to training data: {dates_available}/{len(df_training)} examples have dates\")\n",
    "            \n",
    "            if dates_available == 0:\n",
    "                raise ValueError(\"No dates available. Cannot perform time-based split.\")\n",
    "        else:\n",
    "            raise NameError(\"df_cleaned not found. Cannot add dates for splitting.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not add Date column: {e}\")\n",
    "        print(\"Falling back to alternative split method (by index)\")\n",
    "        # Fallback: split by index (not ideal, but works if dates unavailable)\n",
    "        df_training['Date'] = pd.NaT\n",
    "    \n",
    "    # Remove rows with missing dates for time-based split\n",
    "    df_training_dated = df_training[df_training['Date'].notna()].copy()\n",
    "    \n",
    "    if len(df_training_dated) == 0:\n",
    "        raise ValueError(\"No examples with dates. Cannot perform time-based split.\")\n",
    "    \n",
    "    # Sort by date to ensure chronological order\n",
    "    df_training_dated = df_training_dated.sort_values('Date').reset_index(drop=True)\n",
    "    \n",
    "    # Calculate split indices\n",
    "    total_examples = len(df_training_dated)\n",
    "    \n",
    "    if total_examples < 10:\n",
    "        raise ValueError(f\"Dataset too small for train/validation/test split: {total_examples} examples\")\n",
    "    \n",
    "    # Split percentages: 70% train, 15% validation, 15% test\n",
    "    train_size = int(total_examples * 0.70)\n",
    "    val_size = int(total_examples * 0.15)\n",
    "    \n",
    "    # Split indices\n",
    "    train_end = train_size\n",
    "    val_end = train_end + val_size\n",
    "    \n",
    "    # Create splits\n",
    "    df_train = df_training_dated.iloc[:train_end].copy()\n",
    "    df_val = df_training_dated.iloc[train_end:val_end].copy()\n",
    "    df_test = df_training_dated.iloc[val_end:].copy()\n",
    "    \n",
    "    # Validate splits\n",
    "    print(f\"\\nSplit Statistics:\")\n",
    "    print(f\"  Total examples: {total_examples}\")\n",
    "    print(f\"  Training set: {len(df_train)} ({len(df_train)/total_examples*100:.1f}%)\")\n",
    "    print(f\"  Validation set: {len(df_val)} ({len(df_val)/total_examples*100:.1f}%)\")\n",
    "    print(f\"  Test set: {len(df_test)} ({len(df_test)/total_examples*100:.1f}%)\")\n",
    "    \n",
    "    # Validate date ordering (train < validation < test)\n",
    "    train_max_date = df_train['Date'].max()\n",
    "    val_min_date = df_val['Date'].min()\n",
    "    val_max_date = df_val['Date'].max()\n",
    "    test_min_date = df_test['Date'].min()\n",
    "    \n",
    "    print(f\"\\nDate Range Validation:\")\n",
    "    print(f\"  Training: {df_train['Date'].min()} to {train_max_date}\")\n",
    "    print(f\"  Validation: {val_min_date} to {val_max_date}\")\n",
    "    print(f\"  Test: {test_min_date} to {df_test['Date'].max()}\")\n",
    "    \n",
    "    # Check for data leakage\n",
    "    if train_max_date >= val_min_date:\n",
    "        print(f\"  ⚠ Warning: Potential data leakage - train max date >= val min date\")\n",
    "    else:\n",
    "        print(f\"  ✓ No leakage: train max < val min\")\n",
    "    \n",
    "    if val_max_date >= test_min_date:\n",
    "        print(f\"  ⚠ Warning: Potential data leakage - val max date >= test min date\")\n",
    "    else:\n",
    "        print(f\"  ✓ No leakage: val max < test min\")\n",
    "    \n",
    "    # Store splits (drop Date column as it's not needed for training)\n",
    "    # Actually, keep Date for reference but we can drop it later if needed\n",
    "    df_train_final = df_train.copy()\n",
    "    df_val_final = df_val.copy()\n",
    "    df_test_final = df_test.copy()\n",
    "    \n",
    "    print(f\"\\nStep 6 Complete: Data splits created successfully\")\n",
    "    print(f\"  ✓ Training examples: {len(df_train_final)}\")\n",
    "    print(f\"  ✓ Validation examples: {len(df_val_final)}\")\n",
    "    print(f\"  ✓ Test examples: {len(df_test_final)}\")\n",
    "    \n",
    "    # Display sample from each split\n",
    "    print(f\"\\nSample from training set:\")\n",
    "    print(df_train_final.head(3))\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in data splitting: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68bdc8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 7: MODEL ARCHITECTURE SELECTION\n",
      "============================================================\n",
      "\n",
      "Model Architecture:\n",
      "  Approach: Three separate Random Forest models\n",
      "    - Model 1: Predict NextBest3SquatKg\n",
      "    - Model 2: Predict NextBest3BenchKg\n",
      "    - Model 3: Predict NextBest3DeadliftKg\n",
      "\n",
      "Feature columns: 31\n",
      "  Sample features: ['PrevBest3SquatKg', 'PrevBest3BenchKg', 'PrevBest3DeadliftKg', 'PrevTotalKg', 'PRBest3SquatKg']...\n",
      "\n",
      "Training data preparation:\n",
      "  Total training examples: 118776\n",
      "  Squat examples: 118770 (dropped 6)\n",
      "  Bench examples: 118769 (dropped 7)\n",
      "  Deadlift examples: 118771 (dropped 5)\n",
      "\n",
      "Step 7 Complete: Model architecture selected and data prepared\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Choose Your Model Architecture\n",
    "# Three separate models (one for squat, one for bench, one for deadlift)\n",
    "# Model Type: Random Forest (good balance of performance and interpretability)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    # Check if splits exist\n",
    "    if 'df_train_final' not in locals() and 'df_train_final' not in globals():\n",
    "        raise NameError(\"df_train_final not found. Please run Step 6 first.\")\n",
    "    \n",
    "    if 'df_val_final' not in locals() and 'df_val_final' not in globals():\n",
    "        raise NameError(\"df_val_final not found. Please run Step 6 first.\")\n",
    "    \n",
    "    if 'df_test_final' not in locals() and 'df_test_final' not in globals():\n",
    "        raise NameError(\"df_test_final not found. Please run Step 6 first.\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"STEP 7: MODEL ARCHITECTURE SELECTION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Prepare feature and target columns\n",
    "    metadata_cols = ['LifterID', 'CompIndex']\n",
    "    target_cols = ['NextBest3SquatKg', 'NextBest3BenchKg', 'NextBest3DeadliftKg']\n",
    "    \n",
    "    # Get feature columns (everything except metadata and targets)\n",
    "    feature_cols = [col for col in df_train_final.columns \n",
    "                    if col not in metadata_cols + target_cols]\n",
    "    \n",
    "    # Remove Date if present (not a feature)\n",
    "    if 'Date' in feature_cols:\n",
    "        feature_cols.remove('Date')\n",
    "    \n",
    "    print(f\"\\nModel Architecture:\")\n",
    "    print(f\"  Approach: Three separate Random Forest models\")\n",
    "    print(f\"    - Model 1: Predict NextBest3SquatKg\")\n",
    "    print(f\"    - Model 2: Predict NextBest3BenchKg\")\n",
    "    print(f\"    - Model 3: Predict NextBest3DeadliftKg\")\n",
    "    print(f\"\\nFeature columns: {len(feature_cols)}\")\n",
    "    print(f\"  Sample features: {feature_cols[:5]}...\")\n",
    "    \n",
    "    # Prepare training data\n",
    "    X_train = df_train_final[feature_cols].copy()\n",
    "    y_train_squat = df_train_final['NextBest3SquatKg'].copy()\n",
    "    y_train_bench = df_train_final['NextBest3BenchKg'].copy()\n",
    "    y_train_deadlift = df_train_final['NextBest3DeadliftKg'].copy()\n",
    "    \n",
    "    # Prepare validation data\n",
    "    X_val = df_val_final[feature_cols].copy()\n",
    "    y_val_squat = df_val_final['NextBest3SquatKg'].copy()\n",
    "    y_val_bench = df_val_final['NextBest3BenchKg'].copy()\n",
    "    y_val_deadlift = df_val_final['NextBest3DeadliftKg'].copy()\n",
    "    \n",
    "    # Prepare test data\n",
    "    X_test = df_test_final[feature_cols].copy()\n",
    "    y_test_squat = df_test_final['NextBest3SquatKg'].copy()\n",
    "    y_test_bench = df_test_final['NextBest3BenchKg'].copy()\n",
    "    y_test_deadlift = df_test_final['NextBest3DeadliftKg'].copy()\n",
    "    \n",
    "    # Handle missing target values - drop rows where target is missing\n",
    "    # For each model, only use examples where that target exists\n",
    "    print(f\"\\nTraining data preparation:\")\n",
    "    print(f\"  Total training examples: {len(X_train)}\")\n",
    "    \n",
    "    train_mask_squat = y_train_squat.notna()\n",
    "    train_mask_bench = y_train_bench.notna()\n",
    "    train_mask_deadlift = y_train_deadlift.notna()\n",
    "    \n",
    "    print(f\"  Squat examples: {train_mask_squat.sum()} (dropped {len(X_train) - train_mask_squat.sum()})\")\n",
    "    print(f\"  Bench examples: {train_mask_bench.sum()} (dropped {len(X_train) - train_mask_bench.sum()})\")\n",
    "    print(f\"  Deadlift examples: {train_mask_deadlift.sum()} (dropped {len(X_train) - train_mask_deadlift.sum()})\")\n",
    "    \n",
    "    print(f\"\\nStep 7 Complete: Model architecture selected and data prepared\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in model architecture selection: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d66cac",
   "metadata": {},
   "source": [
    "## Model Architecture Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb1df9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 8: TRAIN MODELS\n",
      "============================================================\n",
      "\n",
      "Random Forest hyperparameters:\n",
      "  n_estimators: 100\n",
      "  max_depth: 10\n",
      "  random_state: 42\n",
      "\n",
      "Training Squat model...\n",
      "  ✓ Trained on 118770 examples\n",
      "\n",
      "Training Bench model...\n",
      "  ✓ Trained on 118769 examples\n",
      "\n",
      "Training Deadlift model...\n",
      "  ✓ Trained on 118771 examples\n",
      "\n",
      "Making predictions on validation set...\n",
      "  ✓ Validation predictions made\n",
      "\n",
      "Step 8 Complete: All three models trained successfully\n",
      "  Models ready for evaluation\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Train the Models\n",
    "# Train three XGBoost models (one for each lift)\n",
    "# Use default hyperparameters for initial training\n",
    "\n",
    "try:\n",
    "    # Check if data is prepared\n",
    "    if 'X_train' not in locals() and 'X_train' not in globals():\n",
    "        raise NameError(\"X_train not found. Please run Step 7 first.\")\n",
    "    \n",
    "    from xgboost import XGBRegressor\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"STEP 8: TRAIN MODELS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Initialize models with default hyperparameters\n",
    "    xgb_base_params = {\n",
    "        'n_estimators': 300,\n",
    "        'learning_rate': 0.1,\n",
    "        'max_depth': 6,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'mae',\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'predictor': 'gpu_predictor',\n",
    "        'device': 'cuda',\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nXGBoost hyperparameters:\")\n",
    "    for k, v in xgb_base_params.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    \n",
    "    # Train Squat Model\n",
    "    print(f\"\\nTraining Squat model...\")\n",
    "    X_train_squat = X_train[train_mask_squat].copy()\n",
    "    y_train_squat_clean = y_train_squat[train_mask_squat].copy()\n",
    "    \n",
    "    model_squat = XGBRegressor(**xgb_base_params)\n",
    "    model_squat.fit(X_train_squat, y_train_squat_clean, verbose=False)\n",
    "    print(f\"  ✓ Trained on {len(X_train_squat)} examples\")\n",
    "    \n",
    "    # Train Bench Model\n",
    "    print(f\"\\nTraining Bench model...\")\n",
    "    X_train_bench = X_train[train_mask_bench].copy()\n",
    "    y_train_bench_clean = y_train_bench[train_mask_bench].copy()\n",
    "    \n",
    "    model_bench = XGBRegressor(**xgb_base_params)\n",
    "    model_bench.fit(X_train_bench, y_train_bench_clean, verbose=False)\n",
    "    print(f\"  ✓ Trained on {len(X_train_bench)} examples\")\n",
    "    \n",
    "    # Train Deadlift Model\n",
    "    print(f\"\\nTraining Deadlift model...\")\n",
    "    X_train_deadlift = X_train[train_mask_deadlift].copy()\n",
    "    y_train_deadlift_clean = y_train_deadlift[train_mask_deadlift].copy()\n",
    "    \n",
    "    model_deadlift = XGBRegressor(**xgb_base_params)\n",
    "    model_deadlift.fit(X_train_deadlift, y_train_deadlift_clean, verbose=False)\n",
    "    print(f\"  ✓ Trained on {len(X_train_deadlift)} examples\")\n",
    "    \n",
    "    # Make predictions on validation set for evaluation\n",
    "    print(f\"\\nMaking predictions on validation set...\")\n",
    "    \n",
    "    # Filter validation set to only examples where target exists\n",
    "    val_mask_squat = y_val_squat.notna()\n",
    "    val_mask_bench = y_val_bench.notna()\n",
    "    val_mask_deadlift = y_val_deadlift.notna()\n",
    "    \n",
    "    y_pred_squat_val = model_squat.predict(X_val[val_mask_squat])\n",
    "    y_pred_bench_val = model_bench.predict(X_val[val_mask_bench])\n",
    "    y_pred_deadlift_val = model_deadlift.predict(X_val[val_mask_deadlift])\n",
    "    \n",
    "    y_actual_squat_val = y_val_squat[val_mask_squat].values\n",
    "    y_actual_bench_val = y_val_bench[val_mask_bench].values\n",
    "    y_actual_deadlift_val = y_val_deadlift[val_mask_deadlift].values\n",
    "    \n",
    "    print(f\"  ✓ Validation predictions made\")\n",
    "    \n",
    "    print(f\"\\nStep 8 Complete: All three models trained successfully\")\n",
    "    print(f\"  Models ready for evaluation\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in model training: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749bd394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "HYPERPARAMETER OPTIMIZATION\n",
      "============================================================\n",
      "\n",
      "Parameter search space:\n",
      "  max_depth: [5, 10, 15, 20, None]\n",
      "  min_samples_split: [2, 5, 10]\n",
      "  min_samples_leaf: [1, 2, 4]\n",
      "  max_features: ['sqrt', 'log2', None]\n",
      "\n",
      "Search settings:\n",
      "  search_type: HalvingRandomSearchCV\n",
      "  min_resources (n_estimators): 50\n",
      "  max_resources (n_estimators): 150\n",
      "  factor: 3\n",
      "  cv: 2 (cross-validation folds)\n",
      "  scoring: neg_mean_absolute_error\n",
      "  random_state: 42\n",
      "  final_n_estimators: 300\n",
      "\n",
      "Recording baseline performance from Step 8...\n",
      "  Baseline MAE - Squat: 8.14 kg\n",
      "  Baseline MAE - Bench: 4.68 kg\n",
      "  Baseline MAE - Deadlift: 8.46 kg\n",
      "\n",
      "Starting hyperparameter optimization...\n",
      "  This should be faster than full RandomizedSearchCV\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Optimization: XGBoost Early Stopping (GPU)\n",
    "# Optimize each model using early stopping on validation data\n",
    "\n",
    "import time\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "try:\n",
    "    # Check if training data exists\n",
    "    if 'X_train' not in locals() and 'X_train' not in globals():\n",
    "        raise NameError(\"X_train not found. Please run Step 7 first.\")\n",
    "    \n",
    "    if 'X_val' not in locals() and 'X_val' not in globals():\n",
    "        raise NameError(\"X_val not found. Please run Step 7 first.\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"HYPERPARAMETER OPTIMIZATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # XGBoost search settings (early stopping will pick best iteration)\n",
    "    xgb_params = {\n",
    "        'n_estimators': 2000,\n",
    "        'learning_rate': 0.05,\n",
    "        'max_depth': 6,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'min_child_weight': 1,\n",
    "        'reg_alpha': 0.0,\n",
    "        'reg_lambda': 1.0,\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'mae',\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'predictor': 'gpu_predictor',\n",
    "        'device': 'cuda',\n",
    "        'random_state': 42\n",
    "    }\n",
    "    early_stopping_rounds = 50\n",
    "    \n",
    "    print(f\"\\nXGBoost settings:\")\n",
    "    for param, value in xgb_params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    print(f\"  early_stopping_rounds: {early_stopping_rounds}\")\n",
    "    \n",
    "    # Store baseline performance for comparison\n",
    "    print(f\"\\nRecording baseline performance from Step 8...\")\n",
    "    baseline_metrics = {}\n",
    "    \n",
    "    # Get baseline validation predictions if available\n",
    "    if 'y_pred_squat_val' in locals() or 'y_pred_squat_val' in globals():\n",
    "        from sklearn.metrics import mean_absolute_error\n",
    "        baseline_metrics['squat'] = mean_absolute_error(y_actual_squat_val, y_pred_squat_val)\n",
    "        baseline_metrics['bench'] = mean_absolute_error(y_actual_bench_val, y_pred_bench_val)\n",
    "        baseline_metrics['deadlift'] = mean_absolute_error(y_actual_deadlift_val, y_pred_deadlift_val)\n",
    "        print(f\"  Baseline MAE - Squat: {baseline_metrics['squat']:.2f} kg\")\n",
    "        print(f\"  Baseline MAE - Bench: {baseline_metrics['bench']:.2f} kg\")\n",
    "        print(f\"  Baseline MAE - Deadlift: {baseline_metrics['deadlift']:.2f} kg\")\n",
    "    else:\n",
    "        print(f\"  Warning: Baseline metrics not found, will calculate after optimization\")\n",
    "    \n",
    "    print(f\"\\nStarting XGBoost optimization with early stopping...\")\n",
    "    print(f\"  This should be faster than CV-based search\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in hyperparameter optimization setup: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acba7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "OPTIMIZING MODELS\n",
      "============================================================\n",
      "\n",
      "[1/3] Optimizing Squat model...\n",
      "n_iterations: 2\n",
      "n_required_iterations: 2\n",
      "n_possible_iterations: 2\n",
      "min_resources_: 50\n",
      "max_resources_: 150\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 3\n",
      "n_resources: 50\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 1\n",
      "n_resources: 150\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "  ✓ Completed in 316.1 seconds\n",
      "  Best score (CV): 8.39 kg MAE\n",
      "  Best parameters: {'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': None, 'max_depth': 20, 'n_estimators': 150}\n",
      "\n",
      "[2/3] Optimizing Bench model...\n",
      "n_iterations: 2\n",
      "n_required_iterations: 2\n",
      "n_possible_iterations: 2\n",
      "min_resources_: 50\n",
      "max_resources_: 150\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 3\n",
      "n_resources: 50\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 1\n",
      "n_resources: 150\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 100\u001b[39m\n\u001b[32m     93\u001b[39m rf_bench = RandomForestRegressor(random_state=random_state, n_jobs=\u001b[32m1\u001b[39m)\n\u001b[32m     94\u001b[39m search_bench = search_cls(\n\u001b[32m     95\u001b[39m     rf_bench,\n\u001b[32m     96\u001b[39m     param_grid_for_search,\n\u001b[32m     97\u001b[39m     **search_kwargs\n\u001b[32m     98\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m \u001b[43msearch_bench\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_bench_cv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_bench_cv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m best_params[\u001b[33m'\u001b[39m\u001b[33mbench\u001b[39m\u001b[33m'\u001b[39m] = search_bench.best_params_\n\u001b[32m    103\u001b[39m search_results[\u001b[33m'\u001b[39m\u001b[33mbench\u001b[39m\u001b[33m'\u001b[39m] = search_bench\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\sklearn\\base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\sklearn\\model_selection\\_search_successive_halving.py:253\u001b[39m, in \u001b[36mBaseSuccessiveHalving.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;28mself\u001b[39m._check_input_parameters(\n\u001b[32m    248\u001b[39m     X=X, y=y, split_params=routed_params.splitter.split\n\u001b[32m    249\u001b[39m )\n\u001b[32m    251\u001b[39m \u001b[38;5;28mself\u001b[39m._n_samples_orig = _num_samples(X)\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[38;5;66;03m# Set best_score_: BaseSearchCV does not set it, as refit is a callable\u001b[39;00m\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m.best_score_ = \u001b[38;5;28mself\u001b[39m.cv_results_[\u001b[33m\"\u001b[39m\u001b[33mmean_test_score\u001b[39m\u001b[33m\"\u001b[39m][\u001b[38;5;28mself\u001b[39m.best_index_]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\sklearn\\base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\sklearn\\model_selection\\_search.py:1053\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1047\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1048\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1049\u001b[39m     )\n\u001b[32m   1051\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1055\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1056\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1057\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\sklearn\\model_selection\\_search_successive_halving.py:357\u001b[39m, in \u001b[36mBaseSuccessiveHalving._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m    350\u001b[39m     cv = \u001b[38;5;28mself\u001b[39m._checked_cv_orig\n\u001b[32m    352\u001b[39m more_results = {\n\u001b[32m    353\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33miter\u001b[39m\u001b[33m\"\u001b[39m: [itr] * n_candidates,\n\u001b[32m    354\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mn_resources\u001b[39m\u001b[33m\"\u001b[39m: [n_resources] * n_candidates,\n\u001b[32m    355\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m357\u001b[39m results = \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmore_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmore_results\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    361\u001b[39m n_candidates_to_keep = ceil(n_candidates / \u001b[38;5;28mself\u001b[39m.factor)\n\u001b[32m    362\u001b[39m candidate_params = _top_k(results, n_candidates_to_keep, itr)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\sklearn\\model_selection\\_search.py:999\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    991\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    992\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    993\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    994\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    995\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    996\u001b[39m         )\n\u001b[32m    997\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m999\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1019\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1020\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1021\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1022\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\sklearn\\utils\\parallel.py:91\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     79\u001b[39m warning_filters = (\n\u001b[32m     80\u001b[39m     filters_func() \u001b[38;5;28;01mif\u001b[39;00m filters_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m warnings.filters\n\u001b[32m     81\u001b[39m )\n\u001b[32m     83\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     84\u001b[39m     (\n\u001b[32m     85\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     90\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Step 2-3: Optimize each model separately using XGBoost early stopping\n",
    "\n",
    "try:\n",
    "    # Check if search settings are defined\n",
    "    if 'xgb_params' not in locals() and 'xgb_params' not in globals():\n",
    "        raise NameError(\"xgb_params not found. Please run previous cell first.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"OPTIMIZING MODELS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    best_params = {}\n",
    "    search_results = {}\n",
    "    search_times = {}\n",
    "    best_iterations = {}\n",
    "    \n",
    "    # Prepare train/validation splits once\n",
    "    X_squat_train = X_train[train_mask_squat]\n",
    "    y_squat_train = y_train_squat[train_mask_squat]\n",
    "    X_squat_val = X_val[val_mask_squat]\n",
    "    y_squat_val_clean = y_val_squat[val_mask_squat]\n",
    "    \n",
    "    X_bench_train = X_train[train_mask_bench]\n",
    "    y_bench_train = y_train_bench[train_mask_bench]\n",
    "    X_bench_val = X_val[val_mask_bench]\n",
    "    y_bench_val_clean = y_val_bench[val_mask_bench]\n",
    "    \n",
    "    X_deadlift_train = X_train[train_mask_deadlift]\n",
    "    y_deadlift_train = y_train_deadlift[train_mask_deadlift]\n",
    "    X_deadlift_val = X_val[val_mask_deadlift]\n",
    "    y_deadlift_val_clean = y_val_deadlift[val_mask_deadlift]\n",
    "    \n",
    "    def _check_cv_size(name, X_train_part, X_val_part):\n",
    "        if len(X_train_part) < 100 or len(X_val_part) < 100:\n",
    "            raise ValueError(\n",
    "                f\"Insufficient data for early stopping ({name}): \"\n",
    "                f\"train={len(X_train_part)}, val={len(X_val_part)}\"\n",
    "            )\n",
    "    \n",
    "    _check_cv_size(\"squat\", X_squat_train, X_squat_val)\n",
    "    _check_cv_size(\"bench\", X_bench_train, X_bench_val)\n",
    "    _check_cv_size(\"deadlift\", X_deadlift_train, X_deadlift_val)\n",
    "    \n",
    "    # Optimize Squat Model\n",
    "    print(f\"\\n[1/3] Optimizing Squat model...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        model_squat_es = XGBRegressor(**xgb_params)\n",
    "        model_squat_es.fit(\n",
    "            X_squat_train,\n",
    "            y_squat_train,\n",
    "            eval_set=[(X_squat_val, y_squat_val_clean)],\n",
    "            early_stopping_rounds=early_stopping_rounds,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        best_iter = model_squat_es.best_iteration\n",
    "        best_iterations['squat'] = best_iter\n",
    "        best_params['squat'] = dict(xgb_params)\n",
    "        best_params['squat']['n_estimators'] = best_iter + 1\n",
    "        search_results['squat'] = model_squat_es\n",
    "        search_times['squat'] = time.time() - start_time\n",
    "        \n",
    "        print(f\"  ✓ Completed in {search_times['squat']:.1f} seconds\")\n",
    "        print(f\"  Best iteration: {best_iter}\")\n",
    "        print(f\"  Best score (val MAE): {model_squat_es.best_score:.2f} kg\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error optimizing squat model: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Optimize Bench Model\n",
    "    print(f\"\\n[2/3] Optimizing Bench model...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        model_bench_es = XGBRegressor(**xgb_params)\n",
    "        model_bench_es.fit(\n",
    "            X_bench_train,\n",
    "            y_bench_train,\n",
    "            eval_set=[(X_bench_val, y_bench_val_clean)],\n",
    "            early_stopping_rounds=early_stopping_rounds,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        best_iter = model_bench_es.best_iteration\n",
    "        best_iterations['bench'] = best_iter\n",
    "        best_params['bench'] = dict(xgb_params)\n",
    "        best_params['bench']['n_estimators'] = best_iter + 1\n",
    "        search_results['bench'] = model_bench_es\n",
    "        search_times['bench'] = time.time() - start_time\n",
    "        \n",
    "        print(f\"  ✓ Completed in {search_times['bench']:.1f} seconds\")\n",
    "        print(f\"  Best iteration: {best_iter}\")\n",
    "        print(f\"  Best score (val MAE): {model_bench_es.best_score:.2f} kg\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error optimizing bench model: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Optimize Deadlift Model\n",
    "    print(f\"\\n[3/3] Optimizing Deadlift model...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        model_deadlift_es = XGBRegressor(**xgb_params)\n",
    "        model_deadlift_es.fit(\n",
    "            X_deadlift_train,\n",
    "            y_deadlift_train,\n",
    "            eval_set=[(X_deadlift_val, y_deadlift_val_clean)],\n",
    "            early_stopping_rounds=early_stopping_rounds,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        best_iter = model_deadlift_es.best_iteration\n",
    "        best_iterations['deadlift'] = best_iter\n",
    "        best_params['deadlift'] = dict(xgb_params)\n",
    "        best_params['deadlift']['n_estimators'] = best_iter + 1\n",
    "        search_results['deadlift'] = model_deadlift_es\n",
    "        search_times['deadlift'] = time.time() - start_time\n",
    "        \n",
    "        print(f\"  ✓ Completed in {search_times['deadlift']:.1f} seconds\")\n",
    "        print(f\"  Best iteration: {best_iter}\")\n",
    "        print(f\"  Best score (val MAE): {model_deadlift_es.best_score:.2f} kg\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error optimizing deadlift model: {e}\")\n",
    "        raise\n",
    "    \n",
    "    total_time = sum(search_times.values())\n",
    "    print(f\"\\n✓ All models optimized in {total_time:.1f} seconds ({total_time/60:.1f} minutes)\")\n",
    "    print(f\"\\nBest iteration summary:\")\n",
    "    for model_name, best_iter in best_iterations.items():\n",
    "        print(f\"  {model_name.capitalize()}: {best_iter}\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in model optimization: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8090ec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Retrain models with optimized hyperparameters on full training set\n",
    "\n",
    "try:\n",
    "    # Check if best parameters exist\n",
    "    if 'best_params' not in locals() and 'best_params' not in globals():\n",
    "        raise NameError(\"best_params not found. Please run optimization cell first.\")\n",
    "    \n",
    "    from xgboost import XGBRegressor\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"RETRAINING MODELS WITH OPTIMIZED HYPERPARAMETERS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Retrain Squat Model\n",
    "    print(f\"\\nRetraining Squat model with optimized parameters...\")\n",
    "    squat_params = dict(best_params['squat'])\n",
    "    model_squat_optimized = XGBRegressor(**squat_params)\n",
    "    model_squat_optimized.fit(X_train[train_mask_squat], y_train_squat[train_mask_squat], verbose=False)\n",
    "    print(f\"  ✓ Trained on {train_mask_squat.sum()} examples\")\n",
    "    \n",
    "    # Retrain Bench Model\n",
    "    print(f\"\\nRetraining Bench model with optimized parameters...\")\n",
    "    bench_params = dict(best_params['bench'])\n",
    "    model_bench_optimized = XGBRegressor(**bench_params)\n",
    "    model_bench_optimized.fit(X_train[train_mask_bench], y_train_bench[train_mask_bench], verbose=False)\n",
    "    print(f\"  ✓ Trained on {train_mask_bench.sum()} examples\")\n",
    "    \n",
    "    # Retrain Deadlift Model\n",
    "    print(f\"\\nRetraining Deadlift model with optimized parameters...\")\n",
    "    deadlift_params = dict(best_params['deadlift'])\n",
    "    model_deadlift_optimized = XGBRegressor(**deadlift_params)\n",
    "    model_deadlift_optimized.fit(X_train[train_mask_deadlift], y_train_deadlift[train_mask_deadlift], verbose=False)\n",
    "    print(f\"  ✓ Trained on {train_mask_deadlift.sum()} examples\")\n",
    "    \n",
    "    # Make predictions on validation set\n",
    "    print(f\"\\nMaking predictions on validation set...\")\n",
    "    y_pred_squat_val_opt = model_squat_optimized.predict(X_val[val_mask_squat])\n",
    "    y_pred_bench_val_opt = model_bench_optimized.predict(X_val[val_mask_bench])\n",
    "    y_pred_deadlift_val_opt = model_deadlift_optimized.predict(X_val[val_mask_deadlift])\n",
    "    \n",
    "    print(f\"  ✓ Validation predictions made\")\n",
    "    \n",
    "    # Update model variables to use optimized versions\n",
    "    model_squat = model_squat_optimized\n",
    "    model_bench = model_bench_optimized\n",
    "    model_deadlift = model_deadlift_optimized\n",
    "    \n",
    "    print(f\"\\n✓ All models retrained with optimized hyperparameters\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error retraining models: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39918830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Compare optimized vs baseline performance\n",
    "\n",
    "try:\n",
    "    # Check if optimized predictions exist\n",
    "    if 'y_pred_squat_val_opt' not in locals() and 'y_pred_squat_val_opt' not in globals():\n",
    "        raise NameError(\"Optimized predictions not found. Please run retraining cell first.\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"PERFORMANCE COMPARISON: BASELINE vs OPTIMIZED\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "    import numpy as np\n",
    "    \n",
    "    # Calculate optimized metrics\n",
    "    optimized_metrics = {}\n",
    "    \n",
    "    optimized_metrics['squat'] = {\n",
    "        'MAE': mean_absolute_error(y_actual_squat_val, y_pred_squat_val_opt),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_actual_squat_val, y_pred_squat_val_opt)),\n",
    "        'R2': r2_score(y_actual_squat_val, y_pred_squat_val_opt)\n",
    "    }\n",
    "    \n",
    "    optimized_metrics['bench'] = {\n",
    "        'MAE': mean_absolute_error(y_actual_bench_val, y_pred_bench_val_opt),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_actual_bench_val, y_pred_bench_val_opt)),\n",
    "        'R2': r2_score(y_actual_bench_val, y_pred_bench_val_opt)\n",
    "    }\n",
    "    \n",
    "    optimized_metrics['deadlift'] = {\n",
    "        'MAE': mean_absolute_error(y_actual_deadlift_val, y_pred_deadlift_val_opt),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_actual_deadlift_val, y_pred_deadlift_val_opt)),\n",
    "        'R2': r2_score(y_actual_deadlift_val, y_pred_deadlift_val_opt)\n",
    "    }\n",
    "    \n",
    "    # Calculate baseline metrics if not already stored\n",
    "    if 'baseline_metrics' not in locals() or len(baseline_metrics) == 0:\n",
    "        baseline_metrics = {}\n",
    "        baseline_metrics['squat'] = mean_absolute_error(y_actual_squat_val, y_pred_squat_val)\n",
    "        baseline_metrics['bench'] = mean_absolute_error(y_actual_bench_val, y_pred_bench_val)\n",
    "        baseline_metrics['deadlift'] = mean_absolute_error(y_actual_deadlift_val, y_pred_deadlift_val)\n",
    "    \n",
    "    # Compare and calculate improvements\n",
    "    print(f\"\\nValidation Set Performance Comparison:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    improvements = {}\n",
    "    \n",
    "    for model_name in ['squat', 'bench', 'deadlift']:\n",
    "        baseline_mae = baseline_metrics[model_name]\n",
    "        optimized_mae = optimized_metrics[model_name]['MAE']\n",
    "        improvement = ((baseline_mae - optimized_mae) / baseline_mae) * 100\n",
    "        \n",
    "        improvements[model_name] = improvement\n",
    "        \n",
    "        print(f\"\\n{model_name.capitalize()} Model:\")\n",
    "        print(f\"  Baseline MAE:  {baseline_mae:.2f} kg\")\n",
    "        print(f\"  Optimized MAE: {optimized_mae:.2f} kg\")\n",
    "        print(f\"  Improvement:   {improvement:+.2f}% ({'✓ Better' if improvement > 0 else '✗ Worse'})\")\n",
    "        print(f\"  Optimized RMSE: {optimized_metrics[model_name]['RMSE']:.2f} kg\")\n",
    "        print(f\"  Optimized R²:   {optimized_metrics[model_name]['R2']:.4f}\")\n",
    "    \n",
    "    # Overall improvement\n",
    "    avg_baseline = np.mean(list(baseline_metrics.values()))\n",
    "    avg_optimized = np.mean([optimized_metrics[m]['MAE'] for m in ['squat', 'bench', 'deadlift']])\n",
    "    avg_improvement = ((avg_baseline - avg_optimized) / avg_baseline) * 100\n",
    "    \n",
    "    print(f\"\\n\" + \"-\" * 60)\n",
    "    print(f\"Overall Average:\")\n",
    "    print(f\"  Baseline MAE:  {avg_baseline:.2f} kg\")\n",
    "    print(f\"  Optimized MAE: {avg_optimized:.2f} kg\")\n",
    "    print(f\"  Average Improvement: {avg_improvement:+.2f}%\")\n",
    "    \n",
    "    # Store for documentation\n",
    "    comparison_results = {\n",
    "        'baseline_metrics': baseline_metrics,\n",
    "        'optimized_metrics': optimized_metrics,\n",
    "        'improvements': improvements,\n",
    "        'avg_improvement': avg_improvement\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n✓ Performance comparison complete\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in performance comparison: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7f8c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary: Hyperparameter Optimization Results\n",
    "\n",
    "try:\n",
    "    # Check if all required variables exist\n",
    "    if 'best_params' not in locals() and 'best_params' not in globals():\n",
    "        raise NameError(\"best_params not found. Please run optimization cells first.\")\n",
    "    \n",
    "    if 'search_times' not in locals() and 'search_times' not in globals():\n",
    "        raise NameError(\"search_times not found. Please run optimization cells first.\")\n",
    "    \n",
    "    if 'comparison_results' not in locals() and 'comparison_results' not in globals():\n",
    "        raise NameError(\"comparison_results not found. Please run comparison cell first.\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"HYPERPARAMETER OPTIMIZATION SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Best hyperparameters\n",
    "    print(f\"\\nBest Hyperparameters:\")\n",
    "    print(\"-\" * 60)\n",
    "    for model_name in ['squat', 'bench', 'deadlift']:\n",
    "        print(f\"\\n{model_name.capitalize()} Model:\")\n",
    "        for param, value in best_params[model_name].items():\n",
    "            print(f\"  {param}: {value}\")\n",
    "    \n",
    "    # Search time summary\n",
    "    print(f\"\\nSearch Time Summary:\")\n",
    "    print(\"-\" * 60)\n",
    "    total_time = sum(search_times.values())\n",
    "    for model_name, time_taken in search_times.items():\n",
    "        print(f\"  {model_name.capitalize()}: {time_taken:.1f} seconds ({time_taken/60:.1f} minutes)\")\n",
    "    print(f\"  Total: {total_time:.1f} seconds ({total_time/60:.1f} minutes)\")\n",
    "    \n",
    "    # Early stopping results\n",
    "    print(f\"\\nEarly Stopping Results:\")\n",
    "    print(\"-\" * 60)\n",
    "    if 'search_results' in locals() or 'search_results' in globals():\n",
    "        for model_name in ['squat', 'bench', 'deadlift']:\n",
    "            model_obj = search_results[model_name]\n",
    "            best_iter = model_obj.best_iteration\n",
    "            best_score = model_obj.best_score\n",
    "            print(f\"  {model_name.capitalize()}: best_iteration={best_iter}, best_val_mae={best_score:.2f} kg\")\n",
    "    \n",
    "    # Performance summary\n",
    "    print(f\"\\nPerformance Summary:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Average Improvement: {comparison_results['avg_improvement']:+.2f}%\")\n",
    "    print(f\"\\nPer-Model Improvements:\")\n",
    "    for model_name, improvement in comparison_results['improvements'].items():\n",
    "        status = \"✓ Improved\" if improvement > 0 else \"✗ Degraded\"\n",
    "        print(f\"  {model_name.capitalize()}: {improvement:+.2f}% ({status})\")\n",
    "    \n",
    "    # Final recommendations\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"OPTIMIZATION COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if comparison_results['avg_improvement'] > 0:\n",
    "        print(f\"✓ Hyperparameter optimization improved model performance\")\n",
    "        print(f\"  Average MAE reduced by {abs(comparison_results['avg_improvement']):.2f}%\")\n",
    "    else:\n",
    "        print(f\"⚠ Hyperparameter optimization did not improve performance\")\n",
    "        print(f\"  Consider: expanding search space, trying different algorithms, or checking for overfitting\")\n",
    "    \n",
    "    print(f\"\\nOptimized models are now ready for final evaluation on test set.\")\n",
    "    print(f\"Run Step 9 to evaluate optimized models on test data.\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in summary generation: {e}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
